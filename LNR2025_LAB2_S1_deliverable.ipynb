{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lab 2:  Sexism Identification in Twitter</h1>\n",
    "<h2 align=\"center\">Session 1. Machine Learning and Feature Engineering</h2>\n",
    "\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put your names here\n",
    "\n",
    "- Kacper Multan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Uexir0NmwwBi"
   },
   "outputs": [],
   "source": [
    "# Reading the entire dataset for both languages and considering only the hard labels. In this lab we do not address the sexism identification task from a Learning with Disagreement (LwD) perspective.\n",
    "\n",
    "from readerEXIST2025 import EXISTReader\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "reader_train = EXISTReader(\"EXIST_2025_Dataset_V0.2/EXIST2025_training.json\")\n",
    "reader_dev = EXISTReader(\"EXIST_2025_Dataset_V0.2/EXIST2025_dev.json\")\n",
    "\n",
    "EnTrainTask1, EnDevTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\")\n",
    "EnTrainTask2, EnDevTask2 = reader_train.get(lang=\"EN\", subtask=\"2\"), reader_dev.get(lang=\"EN\", subtask=\"2\")\n",
    "\n",
    "SpTrainTask1, SpDevTask1 = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\")\n",
    "SpTrainTask2, SpDevTask2 = reader_train.get(lang=\"ES\", subtask=\"2\"), reader_dev.get(lang=\"ES\", subtask=\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/zzzdream/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_re = re.compile(r\"https?:\\/\\/[^\\s]+\", re.U)\n",
    "user_re = re.compile(r\"(@\\w+\\-?(?:\\w+)?)\", re.U)\n",
    "hashtag_re = re.compile(r\"(#\\w+\\-?(?:\\w+)?)\", re.U)\n",
    "\n",
    "stopw = {\n",
    "    \"english\": nltk.corpus.stopwords.words(\"english\"),\n",
    "    \"spanish\": nltk.corpus.stopwords.words(\"spanish\")\n",
    "}\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - tokenization, removal of stopwords, lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lang):\n",
    "    def preprocessing(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        tokens = text.split()\n",
    "        tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words(lang)]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    return text.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_list, lang=\"english\"):\n",
    "    def preprocess(text):\n",
    "        text = web_re.sub(\"\", text)\n",
    "        text = user_re.sub(\"\", text)\n",
    "        text = hashtag_re.sub(\"\", text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    token_list = []\n",
    "    for text in text_list:\n",
    "        text = preprocess(text)\n",
    "        tokens = word_tokenize(text, language=lang)\n",
    "        tokens = [word for word in tokens if word.isalnum() and word not in stopw[lang]]\n",
    "        token_list.append(tokens)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text representation - static embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(tokens, model, dim=300):\n",
    "    zero_vec = np.zeros(dim)\n",
    "    avg_vec = np.zeros(dim)\n",
    "    total_words = 0\n",
    "    for word in tokens:\n",
    "        if word in model:\n",
    "            avg_vec += model[word]\n",
    "            total_words += 1\n",
    "\n",
    "    if total_words == 0:\n",
    "        return zero_vec\n",
    "    return avg_vec / total_words\n",
    "\n",
    "def gensim_sentence_rep(model, tokens_list):\n",
    "    embeddings = []\n",
    "    for tokens in tokens_list:\n",
    "        embeddings.append(get_sentence_vector(tokens, model))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mapping = {\"NO\": 0, \"YES\": 1}\n",
    "y_train_en_task1 = EnTrainTask1[2].map(binary_mapping)\n",
    "y_test_en_task1 = EnDevTask1[2].map(binary_mapping)\n",
    "\n",
    "multi_mapping = {\"DIRECT\": 0, \"REPORTED\": 1, \"JUDGEMENTAL\": 2}\n",
    "y_train_en_task2 = EnTrainTask2[2].map(multi_mapping)\n",
    "y_test_en_task2 = EnDevTask2[2].map(multi_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - tokenization, removal of stopwords, special characters and lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_training_processed_text_task1 = preprocess_text(EnTrainTask1[1], \"english\")\n",
    "en_test_processed_text_task1 = preprocess_text(EnDevTask1[1], \"english\")\n",
    "\n",
    "en_training_processed_text_task2 = preprocess_text(EnTrainTask2[1], \"english\")\n",
    "en_test_processed_text_task2 = preprocess_text(EnDevTask2[1], \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_train_en_task1 = tokenize(EnTrainTask1[1], \"english\")\n",
    "tokenized_text_test_en_task1 = tokenize(EnDevTask1[1], \"english\")\n",
    "\n",
    "tokenized_text_train_en_task2 = tokenize(EnTrainTask2[1], \"english\")\n",
    "tokenized_text_test_en_task2 = tokenize(EnDevTask2[1], \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text representation - traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_en_tfidf_task1 = tfidf_vectorizer.fit_transform(en_training_processed_text_task1)\n",
    "X_test_en_tfidf_task1 = tfidf_vectorizer.transform(en_test_processed_text_task1)\n",
    "\n",
    "X_train_en_tfidf_task2 = tfidf_vectorizer.fit_transform(en_training_processed_text_task2)\n",
    "X_test_en_tfidf_task2 = tfidf_vectorizer.transform(en_test_processed_text_task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text representation - static word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings_en_task1 = gensim_sentence_rep(glove_model, tokenized_text_train_en_task1)\n",
    "X_test_embeddings_en_task1 = gensim_sentence_rep(glove_model, tokenized_text_test_en_task1)\n",
    "\n",
    "X_train_embeddings_en_task2 = gensim_sentence_rep(glove_model, tokenized_text_train_en_task2)\n",
    "X_test_embeddings_en_task2 = gensim_sentence_rep(glove_model, tokenized_text_test_en_task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tfidf = DecisionTreeClassifier()\n",
    "dt_tfidf.fit(X_train_en_tfidf_task1, y_train_en_task1)\n",
    "en_task1_tree_y_predicted_traditional = dt_tfidf.predict(X_test_en_tfidf_task1)\n",
    "\n",
    "dt_tfidf = DecisionTreeClassifier()\n",
    "dt_tfidf.fit(X_train_en_tfidf_task2, y_train_en_task2)\n",
    "en_task2_tree_y_predicted_traditional = dt_tfidf.predict(X_test_en_tfidf_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_embeddings = DecisionTreeClassifier()\n",
    "dt_embeddings.fit(X_train_embeddings_en_task1, y_train_en_task1)\n",
    "en_task1_tree_y_predicted_embeddings = dt_embeddings.predict(X_test_embeddings_en_task1)\n",
    "\n",
    "dt_embeddings = DecisionTreeClassifier()\n",
    "dt_embeddings.fit(X_train_embeddings_en_task2, y_train_en_task2)\n",
    "en_task2_tree_y_predicted_embeddings = dt_embeddings.predict(X_test_embeddings_en_task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def build_mlp_model(input_dim, num_classes=2):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(num_classes if num_classes > 2 else 1, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    loss = 'categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy'\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_classifier(train_embeddings, y_train, test_embeddings, y_test, multiclass=False):\n",
    "    num_classes = len(set(y_train))\n",
    "\n",
    "    if multiclass:\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = build_mlp_model(input_dim=len(train_embeddings[0]), num_classes=num_classes)\n",
    "    model.fit(train_embeddings, y_train, epochs=20, batch_size=8, validation_data=(test_embeddings, y_test))\n",
    "\n",
    "    y_pred_probs = model.predict(test_embeddings)\n",
    "\n",
    "    return y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.5720 - accuracy: 0.7007 - val_loss: 0.5804 - val_accuracy: 0.7207\n",
      "Epoch 2/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.4857 - accuracy: 0.7808 - val_loss: 0.5203 - val_accuracy: 0.7477\n",
      "Epoch 3/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.4394 - accuracy: 0.8042 - val_loss: 0.5817 - val_accuracy: 0.7455\n",
      "Epoch 4/20\n",
      "359/359 [==============================] - 2s 4ms/step - loss: 0.4084 - accuracy: 0.8188 - val_loss: 0.5397 - val_accuracy: 0.7748\n",
      "Epoch 5/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.3687 - accuracy: 0.8411 - val_loss: 0.5711 - val_accuracy: 0.7748\n",
      "Epoch 6/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.3323 - accuracy: 0.8638 - val_loss: 0.5867 - val_accuracy: 0.7635\n",
      "Epoch 7/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.3007 - accuracy: 0.8721 - val_loss: 0.5706 - val_accuracy: 0.7613\n",
      "Epoch 8/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.2676 - accuracy: 0.8923 - val_loss: 0.6729 - val_accuracy: 0.7748\n",
      "Epoch 9/20\n",
      "359/359 [==============================] - 2s 6ms/step - loss: 0.2388 - accuracy: 0.9014 - val_loss: 0.7131 - val_accuracy: 0.7703\n",
      "Epoch 10/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.2080 - accuracy: 0.9160 - val_loss: 0.7311 - val_accuracy: 0.7793\n",
      "Epoch 11/20\n",
      "359/359 [==============================] - 2s 6ms/step - loss: 0.1865 - accuracy: 0.9275 - val_loss: 0.7520 - val_accuracy: 0.7703\n",
      "Epoch 12/20\n",
      "359/359 [==============================] - 2s 4ms/step - loss: 0.1707 - accuracy: 0.9341 - val_loss: 0.8937 - val_accuracy: 0.7635\n",
      "Epoch 13/20\n",
      "359/359 [==============================] - 2s 6ms/step - loss: 0.1543 - accuracy: 0.9383 - val_loss: 1.0083 - val_accuracy: 0.7748\n",
      "Epoch 14/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.1434 - accuracy: 0.9453 - val_loss: 1.0346 - val_accuracy: 0.7613\n",
      "Epoch 15/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 1.1005 - val_accuracy: 0.7703\n",
      "Epoch 16/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.1088 - accuracy: 0.9585 - val_loss: 1.2569 - val_accuracy: 0.7568\n",
      "Epoch 17/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.1091 - accuracy: 0.9606 - val_loss: 1.2208 - val_accuracy: 0.7680\n",
      "Epoch 18/20\n",
      "359/359 [==============================] - 2s 5ms/step - loss: 0.1052 - accuracy: 0.9599 - val_loss: 1.3223 - val_accuracy: 0.7635\n",
      "Epoch 19/20\n",
      "359/359 [==============================] - 2s 4ms/step - loss: 0.1028 - accuracy: 0.9613 - val_loss: 1.2802 - val_accuracy: 0.7770\n",
      "Epoch 20/20\n",
      "359/359 [==============================] - 2s 4ms/step - loss: 0.0911 - accuracy: 0.9683 - val_loss: 1.3068 - val_accuracy: 0.7613\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Epoch 1/20\n",
      "107/107 [==============================] - 1s 6ms/step - loss: 0.8803 - accuracy: 0.6215 - val_loss: 0.8855 - val_accuracy: 0.6301\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.7825 - accuracy: 0.6636 - val_loss: 0.8767 - val_accuracy: 0.6164\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.7364 - accuracy: 0.6881 - val_loss: 0.8894 - val_accuracy: 0.6438\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.7048 - accuracy: 0.6974 - val_loss: 0.8782 - val_accuracy: 0.6096\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.6462 - accuracy: 0.7220 - val_loss: 0.9048 - val_accuracy: 0.6575\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.5932 - accuracy: 0.7500 - val_loss: 0.9241 - val_accuracy: 0.6370\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.5283 - accuracy: 0.7757 - val_loss: 1.0107 - val_accuracy: 0.6164\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.4683 - accuracy: 0.8084 - val_loss: 1.1327 - val_accuracy: 0.6233\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8154 - val_loss: 1.1031 - val_accuracy: 0.6233\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.3657 - accuracy: 0.8551 - val_loss: 1.1218 - val_accuracy: 0.6096\n",
      "Epoch 11/20\n",
      "107/107 [==============================] - 0s 5ms/step - loss: 0.2759 - accuracy: 0.8960 - val_loss: 1.4205 - val_accuracy: 0.6370\n",
      "Epoch 12/20\n",
      "107/107 [==============================] - 1s 5ms/step - loss: 0.2492 - accuracy: 0.9171 - val_loss: 1.4632 - val_accuracy: 0.5822\n",
      "Epoch 13/20\n",
      "107/107 [==============================] - 1s 5ms/step - loss: 0.2031 - accuracy: 0.9311 - val_loss: 1.7800 - val_accuracy: 0.6027\n",
      "Epoch 14/20\n",
      "107/107 [==============================] - 1s 5ms/step - loss: 0.1889 - accuracy: 0.9241 - val_loss: 1.8250 - val_accuracy: 0.5959\n",
      "Epoch 15/20\n",
      "107/107 [==============================] - 0s 5ms/step - loss: 0.1487 - accuracy: 0.9509 - val_loss: 1.8105 - val_accuracy: 0.5959\n",
      "Epoch 16/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9568 - val_loss: 1.9057 - val_accuracy: 0.6096\n",
      "Epoch 17/20\n",
      "107/107 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9685 - val_loss: 1.9759 - val_accuracy: 0.6096\n",
      "Epoch 18/20\n",
      "107/107 [==============================] - 0s 5ms/step - loss: 0.0920 - accuracy: 0.9720 - val_loss: 2.1700 - val_accuracy: 0.6096\n",
      "Epoch 19/20\n",
      "107/107 [==============================] - 0s 5ms/step - loss: 0.0870 - accuracy: 0.9708 - val_loss: 2.2218 - val_accuracy: 0.5822\n",
      "Epoch 20/20\n",
      "107/107 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9778 - val_loss: 2.2347 - val_accuracy: 0.5959\n",
      "5/5 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "en_task1_mlp_y_predicted = train_classifier(np.array(X_train_embeddings_en_task1), np.array(y_train_en_task1), np.array(X_test_embeddings_en_task1), np.array(y_test_en_task1))\n",
    "en_task2_mlp_y_predicted = train_classifier(np.array(X_train_embeddings_en_task2), np.array(y_train_en_task2), np.array(X_test_embeddings_en_task2), np.array(y_test_en_task2), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Ensemble of Decision Trees - Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tfidf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_tfidf.fit(X_train_en_tfidf_task1, y_train_en_task1)\n",
    "en_task1_forest_y_predicted_traditional = rf_tfidf.predict(X_test_en_tfidf_task1)\n",
    "\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_tfidf.fit(X_train_en_tfidf_task2, y_train_en_task2)\n",
    "en_task2_forest_y_predicted_traditional = rf_tfidf.predict(X_test_en_tfidf_task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking - Multilayer Perceptron + Decision Tree + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=15, random_state=42)\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[('dt', dt_model), ('mlp', mlp_model), ('svm', svm_model)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "stacked_model.fit(X_train_en_tfidf_task1, y_train_en_task1)\n",
    "en_task1_stacking_y_predicted_traditional = stacked_model.predict(X_test_en_tfidf_task1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=15, random_state=42)\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[('dt', dt_model), ('mlp', mlp_model), ('svm', svm_model)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "stacked_model.fit(X_train_en_tfidf_task2, y_train_en_task2)\n",
    "en_task2_stacking_y_predicted_traditional = stacked_model.predict(X_test_en_tfidf_task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(y_predicted, y_test, model_type, multiclass=False, probabilities=False):\n",
    "    if multiclass:\n",
    "        if probabilities:\n",
    "            y_predicted = y_predicted.argmax(axis=1)\n",
    "        report = f1_score(y_test, y_predicted, average='macro', zero_division=0)\n",
    "    else:\n",
    "        y_predicted = (y_predicted > 0.5).astype(int)\n",
    "        report = f1_score(y_test, y_predicted, pos_label=1, zero_division=0)\n",
    "\n",
    "    print(model_type)\n",
    "    print(\"------------------------\\n\")\n",
    "    print(\"Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\" if multiclass else \"Binary Classification Report (Sexist vs. Non-Sexist)\")\n",
    "    print('F1 score: ', report, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Perceptron - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.7087912087912088 \n",
      "\n",
      "Multilayer Perceptron - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.4610975317893448 \n",
      "\n",
      "Decision Tree Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.6997389033942558 \n",
      "\n",
      "Decision Tree Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.38909284384144716 \n",
      "\n",
      "Decision Tree Classifier - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.5570291777188329 \n",
      "\n",
      "Decision Tree Classifier - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.3665978829913256 \n",
      "\n",
      "Random Forest Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.732394366197183 \n",
      "\n",
      "Random Forest Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.33659116170326936 \n",
      "\n",
      "Stacking - MLP + Decision Tree + SVM - Traditional\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.7457627118644068 \n",
      "\n",
      "Stacking - MLP + Decision Tree - Traditional\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.35713863784039224 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(en_task1_mlp_y_predicted, y_test_en_task1, \"Multilayer Perceptron - Static Embeddings\")\n",
    "show_results(en_task2_mlp_y_predicted, y_test_en_task2, \"Multilayer Perceptron - Static Embeddings\", True, True)\n",
    "\n",
    "show_results(en_task1_tree_y_predicted_traditional, y_test_en_task1, \"Decision Tree Classifier - Traditional\")\n",
    "show_results(en_task2_tree_y_predicted_traditional, y_test_en_task2, \"Decision Tree Classifier - Traditional\", True)\n",
    "\n",
    "show_results(en_task1_tree_y_predicted_embeddings, y_test_en_task1, \"Decision Tree Classifier - Static Embeddings\")\n",
    "show_results(en_task2_tree_y_predicted_embeddings, y_test_en_task2, \"Decision Tree Classifier - Static Embeddings\", True)\n",
    "\n",
    "show_results(en_task1_forest_y_predicted_traditional, y_test_en_task1, \"Random Forest Classifier - Traditional\")\n",
    "show_results(en_task2_forest_y_predicted_traditional, y_test_en_task2, \"Random Forest Classifier - Traditional\", True)\n",
    "\n",
    "show_results(en_task1_stacking_y_predicted_traditional, y_test_en_task1, \"Stacking - MLP + Decision Tree + SVM - Traditional\")\n",
    "show_results(en_task2_stacking_y_predicted_traditional, y_test_en_task2, \"Stacking - MLP + Decision Tree - Traditional\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPANISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mapping = {\"NO\": 0, \"YES\": 1}\n",
    "y_train_sp_task1 = SpTrainTask1[2].map(binary_mapping)\n",
    "y_test_sp_task1 = SpDevTask1[2].map(binary_mapping)\n",
    "\n",
    "multi_mapping = {\"DIRECT\": 0, \"REPORTED\": 1, \"JUDGEMENTAL\": 2}\n",
    "y_train_sp_task2 = SpTrainTask2[2].map(multi_mapping)\n",
    "y_test_sp_task2 = SpDevTask2[2].map(multi_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_training_processed_text_task1 = preprocess_text(SpTrainTask1[1], \"spanish\")\n",
    "sp_test_processed_text_task1 = preprocess_text(SpDevTask1[1], \"spanish\")\n",
    "\n",
    "sp_training_processed_text_task2 = preprocess_text(SpTrainTask2[1], \"spanish\")\n",
    "sp_test_processed_text_task2 = preprocess_text(SpDevTask2[1], \"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_train_sp_task1 = tokenize(SpTrainTask1[1], \"spanish\")\n",
    "tokenized_text_test_sp_task1 = tokenize(SpDevTask1[1], \"spanish\")\n",
    "\n",
    "tokenized_text_train_sp_task2 = tokenize(SpTrainTask2[1], \"spanish\")\n",
    "tokenized_text_test_sp_task2 = tokenize(SpDevTask2[1], \"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet representations (Feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_sp_tfidf_task1 = tfidf_vectorizer.fit_transform(sp_training_processed_text_task1)\n",
    "X_test_sp_tfidf_task1 = tfidf_vectorizer.transform(sp_test_processed_text_task1)\n",
    "\n",
    "X_train_sp_tfidf_task2 = tfidf_vectorizer.fit_transform(sp_training_processed_text_task2)\n",
    "X_test_sp_tfidf_task2 = tfidf_vectorizer.transform(sp_test_processed_text_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-25 15:46:25--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.41.96, 18.154.41.57, 18.154.41.8, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.154.41.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
      "Saving to: ‘cc.en.300.vec.gz.2’\n",
      "\n",
      "cc.en.300.vec.gz.2  100%[===================>]   1.23G  23.9MB/s    in 35s     \n",
      "\n",
      "2025-03-25 15:47:01 (36.0 MB/s) - ‘cc.en.300.vec.gz.2’ saved [1325960915/1325960915]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_sp_model = KeyedVectors.load_word2vec_format(\"cc.en.300.vec.gz\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings_sp_task1 = gensim_sentence_rep(fasttext_sp_model, tokenized_text_train_sp_task1)\n",
    "X_test_embeddings_sp_task1 = gensim_sentence_rep(fasttext_sp_model, tokenized_text_test_sp_task1)\n",
    "\n",
    "X_train_embeddings_sp_task2 = gensim_sentence_rep(fasttext_sp_model, tokenized_text_train_sp_task2)\n",
    "X_test_embeddings_sp_task2 = gensim_sentence_rep(fasttext_sp_model, tokenized_text_test_sp_task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tfidf = DecisionTreeClassifier()\n",
    "dt_tfidf.fit(X_train_sp_tfidf_task1, y_train_sp_task1)\n",
    "sp_task1_tree_y_predicted_traditional = dt_tfidf.predict(X_test_sp_tfidf_task1)\n",
    "\n",
    "dt_tfidf = DecisionTreeClassifier()\n",
    "dt_tfidf.fit(X_train_sp_tfidf_task2, y_train_sp_task2)\n",
    "sp_task2_tree_y_predicted_traditional = dt_tfidf.predict(X_test_sp_tfidf_task2)\n",
    "\n",
    "dt_embeddings = DecisionTreeClassifier()\n",
    "dt_embeddings.fit(X_train_embeddings_sp_task1, y_train_sp_task1)\n",
    "sp_task1_tree_y_predicted_embeddings = dt_embeddings.predict(X_test_embeddings_sp_task1)\n",
    "\n",
    "dt_embeddings = DecisionTreeClassifier()\n",
    "dt_embeddings.fit(X_train_embeddings_sp_task2, y_train_sp_task2)\n",
    "sp_task2_tree_y_predicted_embeddings = dt_embeddings.predict(X_test_embeddings_sp_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200/200 [==============================] - 2s 4ms/step - loss: 0.6760 - accuracy: 0.5717 - val_loss: 0.6653 - val_accuracy: 0.5735\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6619 - val_loss: 0.6054 - val_accuracy: 0.6673\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5982 - accuracy: 0.6850 - val_loss: 0.5715 - val_accuracy: 0.7020\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5725 - accuracy: 0.6982 - val_loss: 0.5683 - val_accuracy: 0.7143\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5569 - accuracy: 0.7138 - val_loss: 0.5674 - val_accuracy: 0.7082\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.5490 - accuracy: 0.7210 - val_loss: 0.5721 - val_accuracy: 0.7000\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5284 - accuracy: 0.7379 - val_loss: 0.5901 - val_accuracy: 0.6959\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5212 - accuracy: 0.7433 - val_loss: 0.5718 - val_accuracy: 0.7122\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5042 - accuracy: 0.7483 - val_loss: 0.5987 - val_accuracy: 0.6939\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4957 - accuracy: 0.7630 - val_loss: 0.5841 - val_accuracy: 0.7041\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4837 - accuracy: 0.7696 - val_loss: 0.5906 - val_accuracy: 0.6898\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4669 - accuracy: 0.7743 - val_loss: 0.6145 - val_accuracy: 0.7020\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4608 - accuracy: 0.7752 - val_loss: 0.6115 - val_accuracy: 0.6857\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4375 - accuracy: 0.8046 - val_loss: 0.6103 - val_accuracy: 0.7082\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4362 - accuracy: 0.7996 - val_loss: 0.6019 - val_accuracy: 0.7000\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4212 - accuracy: 0.8018 - val_loss: 0.6040 - val_accuracy: 0.6898\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3978 - accuracy: 0.8143 - val_loss: 0.6487 - val_accuracy: 0.6776\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3885 - accuracy: 0.8222 - val_loss: 0.6529 - val_accuracy: 0.6878\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3716 - accuracy: 0.8331 - val_loss: 0.6895 - val_accuracy: 0.6796\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3662 - accuracy: 0.8406 - val_loss: 0.6637 - val_accuracy: 0.6918\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch 1/20\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 0.9532 - accuracy: 0.5933 - val_loss: 0.9724 - val_accuracy: 0.5604\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8994 - accuracy: 0.6048 - val_loss: 0.9559 - val_accuracy: 0.5604\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8781 - accuracy: 0.6105 - val_loss: 0.9528 - val_accuracy: 0.5652\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8473 - accuracy: 0.6187 - val_loss: 0.9522 - val_accuracy: 0.5700\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8211 - accuracy: 0.6294 - val_loss: 0.9678 - val_accuracy: 0.5700\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.8135 - accuracy: 0.6360 - val_loss: 0.9528 - val_accuracy: 0.5700\n",
      "Epoch 7/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7872 - accuracy: 0.6565 - val_loss: 0.9494 - val_accuracy: 0.5652\n",
      "Epoch 8/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7741 - accuracy: 0.6426 - val_loss: 0.9755 - val_accuracy: 0.5700\n",
      "Epoch 9/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7604 - accuracy: 0.6623 - val_loss: 0.9753 - val_accuracy: 0.5604\n",
      "Epoch 10/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7570 - accuracy: 0.6647 - val_loss: 0.9968 - val_accuracy: 0.5845\n",
      "Epoch 11/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7405 - accuracy: 0.6836 - val_loss: 0.9960 - val_accuracy: 0.5797\n",
      "Epoch 12/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.7292 - accuracy: 0.6845 - val_loss: 0.9826 - val_accuracy: 0.5411\n",
      "Epoch 13/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.6993 - val_loss: 1.0364 - val_accuracy: 0.5700\n",
      "Epoch 14/20\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6805 - accuracy: 0.7182 - val_loss: 1.0210 - val_accuracy: 0.5700\n",
      "Epoch 15/20\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6659 - accuracy: 0.7099 - val_loss: 1.0494 - val_accuracy: 0.5894\n",
      "Epoch 16/20\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6460 - accuracy: 0.7321 - val_loss: 1.0917 - val_accuracy: 0.5700\n",
      "Epoch 17/20\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.6263 - accuracy: 0.7280 - val_loss: 1.0907 - val_accuracy: 0.5652\n",
      "Epoch 18/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6092 - accuracy: 0.7461 - val_loss: 1.0220 - val_accuracy: 0.5411\n",
      "Epoch 19/20\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.6138 - accuracy: 0.7379 - val_loss: 1.0785 - val_accuracy: 0.5556\n",
      "Epoch 20/20\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.5670 - accuracy: 0.7740 - val_loss: 1.1566 - val_accuracy: 0.5797\n",
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "sp_task1_mlp_y_predicted = train_classifier(np.array(X_train_embeddings_sp_task1), np.array(y_train_sp_task1), np.array(X_test_embeddings_sp_task1), np.array(y_test_sp_task1))\n",
    "sp_task2_mlp_y_predicted = train_classifier(np.array(X_train_embeddings_sp_task2), np.array(y_train_sp_task2), np.array(X_test_embeddings_sp_task2), np.array(y_test_sp_task2), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tfidf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_tfidf.fit(X_train_sp_tfidf_task1, y_train_sp_task1)\n",
    "sp_task1_forest_y_predicted_traditional = rf_tfidf.predict(X_test_sp_tfidf_task1)\n",
    "\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_tfidf.fit(X_train_sp_tfidf_task2, y_train_sp_task2)\n",
    "sp_task2_forest_y_predicted_traditional = rf_tfidf.predict(X_test_sp_tfidf_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/zzzdream/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=15, random_state=42)\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "stacked_model1 = StackingClassifier(\n",
    "    estimators=[('dt', dt_model), ('mlp', mlp_model), ('svm', svm_model)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "stacked_model1.fit(X_train_embeddings_sp_task1, y_train_sp_task1)\n",
    "sp_task1_stacking_y_predicted_traditional = stacked_model1.predict(X_test_embeddings_sp_task1)\n",
    "\n",
    "stacked_model2 = StackingClassifier(\n",
    "    estimators=[('dt', dt_model), ('mlp', mlp_model), ('svm', svm_model)],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "stacked_model2.fit(X_train_embeddings_sp_task2, y_train_sp_task2)\n",
    "sp_task2_stacking_y_predicted_traditional = stacked_model1.predict(X_test_embeddings_sp_task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Perceptron - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.7145557655954632 \n",
      "\n",
      "Multilayer Perceptron - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.4241071428571428 \n",
      "\n",
      "Decision Tree Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.706959706959707 \n",
      "\n",
      "Decision Tree Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.4303346408609566 \n",
      "\n",
      "Decision Tree Classifier - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.5797665369649806 \n",
      "\n",
      "Decision Tree Classifier - Static Embeddings\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.368358991108051 \n",
      "\n",
      "Random Forest Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.7475915221579961 \n",
      "\n",
      "Random Forest Classifier - Traditional\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.4056654456654456 \n",
      "\n",
      "Stacking - Decision Tree + MLP + SVM\n",
      "------------------------\n",
      "\n",
      "Binary Classification Report (Sexist vs. Non-Sexist)\n",
      "F1 score:  0.7102040816326529 \n",
      "\n",
      "Stacking - Decision Tree + MLP + SVM\n",
      "------------------------\n",
      "\n",
      "Multiclass Classification Report (DIRECT, REPORTED, JUDGEMENTAL)\n",
      "F1 score:  0.21992146384908104 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(sp_task1_mlp_y_predicted, y_test_sp_task1, \"Multilayer Perceptron - Static Embeddings\")\n",
    "show_results(sp_task2_mlp_y_predicted, y_test_sp_task2, \"Multilayer Perceptron - Static Embeddings\", True, True)\n",
    "\n",
    "show_results(sp_task1_tree_y_predicted_traditional, y_test_sp_task1, \"Decision Tree Classifier - Traditional\")\n",
    "show_results(sp_task2_tree_y_predicted_traditional, y_test_sp_task2, \"Decision Tree Classifier - Traditional\", True)\n",
    "\n",
    "show_results(sp_task1_tree_y_predicted_embeddings, y_test_sp_task1, \"Decision Tree Classifier - Static Embeddings\")\n",
    "show_results(sp_task2_tree_y_predicted_embeddings, y_test_sp_task2, \"Decision Tree Classifier - Static Embeddings\", True)\n",
    "\n",
    "show_results(sp_task1_forest_y_predicted_traditional, y_test_sp_task1, \"Random Forest Classifier - Traditional\")\n",
    "show_results(sp_task2_forest_y_predicted_traditional, y_test_sp_task2, \"Random Forest Classifier - Traditional\", True)\n",
    "\n",
    "show_results(sp_task1_stacking_y_predicted_traditional, y_test_sp_task1, \"Stacking - Decision Tree + MLP + SVM\")\n",
    "show_results(sp_task2_stacking_y_predicted_traditional, y_test_sp_task2, \"Stacking - Decision Tree + MLP + SVM\", True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

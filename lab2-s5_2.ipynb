{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24-_ZnyIuShY"
      },
      "source": [
        "<h1 align=\"center\">Lab 2: Sexism Identification in Twitter</h1>\n",
        "<h2 align=\"center\">Session 5. Large Language Models: Prompting and In-context Learning</h2>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp_Qwn3cuShb"
      },
      "source": [
        "### Student Names\n",
        "\n",
        "- Student 1\n",
        "- Student 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6YAOysPuShc"
      },
      "source": [
        "### CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nFj3DmItuShd"
      },
      "outputs": [],
      "source": [
        "COLAB = False  # Set to True if using Google Colab\n",
        "PIP = True    # Set to True if libraries need to be installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6MXclUuuShf"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZTA_gd1_OEz_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: datasets in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (3.5.0)\n",
            "Requirement already satisfied: accelerate in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (69.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: evaluate in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
            "Requirement already satisfied: aiohttp in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: PyEvALL in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (0.1.78)\n",
            "Requirement already satisfied: jsbeautifier==1.14.9 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (1.14.9)\n",
            "Requirement already satisfied: jsonschema==4.23.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (4.23.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (2.2.3)\n",
            "Requirement already satisfied: setuptools==69.5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (69.5.1)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (0.9.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsbeautifier==1.14.9->PyEvALL) (1.17.0)\n",
            "Requirement already satisfied: editorconfig>=0.12.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsbeautifier==1.14.9->PyEvALL) (0.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (0.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas==2.2.3->PyEvALL) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas==2.2.3->PyEvALL) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas==2.2.3->PyEvALL) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema==4.23.0->PyEvALL) (4.13.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "if PIP:\n",
        "    !pip install transformers --upgrade\n",
        "    !pip install datasets accelerate\n",
        "    !pip install evaluate\n",
        "    !pip install -U PyEvALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VxMIGs_euShf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import Dataset\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6LJeq9TuShg"
      },
      "source": [
        "## Import readerEXIST2025 Library and Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RE2_dL-7uShh"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = \"/content/drive/MyDrive/LNR/\"\n",
        "else:\n",
        "    base_path = \"../\"\n",
        "\n",
        "library_path = os.path.join(base_path, \"Lab2-S1\")\n",
        "sys.path.append(library_path)\n",
        "\n",
        "from readerEXIST2025 import EXISTReader\n",
        "\n",
        "dataset_path = os.path.join(base_path, \"corpora/EXIST_2025_Dataset_V0.3/\")\n",
        "file_train = os.path.join(dataset_path, \"EXIST2025_training.json\")\n",
        "file_dev = os.path.join(dataset_path, \"EXIST2025_dev.json\")\n",
        "file_test = os.path.join(dataset_path, \"EXIST2025_test_clean.json\")\n",
        "\n",
        "reader_train = EXISTReader(file_train)\n",
        "reader_dev = EXISTReader(file_dev)\n",
        "reader_test = EXISTReader(file_test)\n",
        "\n",
        "EnTrainTask2 = reader_train.get(lang=\"EN\", subtask=\"2\")\n",
        "EnDevTask2 = reader_dev.get(lang=\"EN\", subtask=\"2\")\n",
        "EnTestTask2 = reader_test.get(lang=\"EN\", subtask=\"2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dUXXawpruShj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 2 files: 100%|██████████| 2/2 [03:46<00:00, 113.35s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 11.26it/s]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ],
      "source": [
        "HF_TOKEN = \"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\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        "    token=HF_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y74tkueEuShj"
      },
      "source": [
        "## Prompt Engineering for Task 2\n",
        "\n",
        "Task 2 involves classifying tweets into categories of sexism: 'IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', or 'NON-SEXIST'. We designed two prompting strategies: **zero-shot** and **few-shot**. The prompts are crafted to ensure clear instructions, consistent output format, and alignment with the task's requirements.\n",
        "\n",
        "### Zero-Shot Prompt\n",
        "The zero-shot prompt provides a clear task description and expects the model to classify without examples.\n",
        "\n",
        "### Few-Shot Prompt\n",
        "The few-shot prompt includes 5 carefully selected examples (one per category except NON-SEXIST, which has two to balance the dataset) to guide the model. Examples were chosen to be representative and unambiguous, sampled from the training set.\n",
        "\n",
        "### Output Format\n",
        "The model is instructed to output only the category name (e.g., 'OBJECTIFICATION') to ensure consistency and ease of evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJNp3UHMewT_"
      },
      "outputs": [],
      "source": [
        "def create_incontext_zero_prompt(task_description, query, context=None, role=None, output=None):\n",
        "    prompt = f\"\"\n",
        "    if role!=None:\n",
        "        prompt+= f\"You are an expert {role}.\\n\\n\"\n",
        "\n",
        "    prompt += f\"Task: {task_description}\\n\\n\"\n",
        "    if context!=None:\n",
        "         prompt+= f\"Context: {context}\\n\\n\"\n",
        "    if output != None:\n",
        "         prompt+= f\"Output Format: {output} \\n\\n\"\n",
        "    # Add query\n",
        "    prompt+=f\"Input: {query}\\n\"\n",
        "    prompt+=f\"Output: \"\n",
        "    return prompt\n",
        "\n",
        "def zero_shot_prompt(tweet):\n",
        "    return f\"\"\"\n",
        "Task: Classify the following tweet into one of the following categories of sexism: 'REPORTED', \"JUDGEMENTAL\" or \"DIRECT\". Return only the category name.\n",
        "\n",
        "Tweet: {tweet}\n",
        "\n",
        "Category:\n",
        "\"\"\"\n",
        "\n",
        "def few_shot_prompt(tweet):\n",
        "    return f\"\"\"\n",
        "Task: Classify the following tweet into one of the following categories of sexism: 'REPORTED', \"JUDGEMENTAL\" or \"DIRECT\". Return only the category name.\n",
        "\n",
        "Tweet: {tweet}\n",
        "\n",
        "\"Examples:\",\n",
        "\"1. Tweet: \"Heard a guy at work say women shouldn't be in tech jobs.\" Category: REPORTED\",\n",
        "\"2. Tweet: \"People who say women can't handle tough jobs are just pathetic and wrong.\" Category: JUDGEMENTAL\",\n",
        "\"3. Tweet: \"Women are too emotional to lead anything serious.\" Category: DIRECT\",\n",
        "\n",
        "Category:\n",
        "\"\"\"\n",
        "\n",
        "# def perform_classification(model, tokenizer, prompt, ntokens=8, nseq=1, temp=0.7):\n",
        "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    # outputs = model.generate(\n",
        "    #     **inputs,\n",
        "    #     max_new_tokens=ntokens,\n",
        "    #     num_return_sequences=nseq,\n",
        "    #     temperature=temp\n",
        "    # )\n",
        "    # response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # # Extract the category from the response\n",
        "    # category = response.split(\"Category:\")[-1].strip()\n",
        "    # valid_categories = [\n",
        "    #     'REPORTED', 'JUDGEMENTAL', 'DIRECT'\n",
        "    # ]\n",
        "    # return category if category in valid_categories else 'UNKNOWN'\n",
        "def perform_incontext_classification(model, tokenizer, prompt, ntokens=8, nseq=1, temp=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=ntokens,\n",
        "        num_return_sequences=nseq,\n",
        "        temperature=temp\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def output_postprocessing_incontext_zero_s2(output):\n",
        "    outputp=output.rsplit(\"Output: \", 1)[-1].strip()\n",
        "    for line in outputp.split('\\n'):\n",
        "        line=line.strip()\n",
        "        if line!=\"\":\n",
        "             if line.upper().startswith(\"DIRECT\"): return \"DIRECT\"\n",
        "             if line.upper().startswith(\"REPORTED\"): return \"REPORTED\"\n",
        "             if line.upper().startswith(\"JUDGEMENTAL\"): return \"JUDGEMENTAL\"\n",
        "    return \"UNK\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y74tkueEuShj"
      },
      "source": [
        "## Prompt Evaluation on Validation Subset\n",
        "\n",
        "We evaluate both prompts on a random subset of 100 samples from the validation dataset (EnDevTask2) using the F1-macro average metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def incontext_zero_pipeline_task2(model, tokenizer, devData, testData, postprocessing, samples = None, **params):\n",
        "\n",
        "    role= \"in social psychology and linguistics with vast experience analyzing social media content and discriminative and harmful language\"\n",
        "    task =\"\"\"Sexism Source Intention in tweets is a three class classification task aims to categorize\n",
        "    the sexist tweets according to the intention of the author. This distinction allow us to differentiate\n",
        "    sexism that is actually taking place online from sexism which is being suffered by women in other situations\n",
        "    but that is being reported in social networks with the aim of complaining and fighting against sexism.\n",
        "    The task include the following ternary classification of tweets.\n",
        "\n",
        "    REPORTED: the intention is to report and share a sexist situation suffered by a woman or women.\n",
        "    JUDGEMENTAL the intention is to condemn and critizise sexist situations or behaviours.\n",
        "    DIRECT: The intention is to write a message that is sexist by itself or incites to be sexist.\n",
        "     \"\"\"\n",
        "    output= \"The output is a single label: REPORTED/JUDGEMENTAL/DIRECT.\"\n",
        "    context=None\n",
        "\n",
        "    ntokens=params.get(\"max_new_tokens\", 8)\n",
        "    nseq=params.get(\"num_return_sequences\", 1)\n",
        "    temp=params.get(\"temperature\", 0.7)\n",
        "\n",
        "    if samples is not None:\n",
        "        devData = devData.select(random.sample(range(len(devData)), samples))\n",
        "        testData = testData.select(random.sample(range(len(testData)), samples))\n",
        "\n",
        "    idqueries= devData[0].tolist()\n",
        "    textqueries= devData[1].tolist()\n",
        "    labelqueries=devData[2].tolist()\n",
        "\n",
        "    predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt, ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in [\"DIRECT\", \"REPORTED\", \"JUDGEMENTAL\"]: predictions.append(pred)\n",
        "        else: predictions.append(\"DIRECT\")\n",
        "        print(\"ID: \", idqueries[i], \"Ground Truth:\", labelqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create dev output DataFrame\n",
        "    dev_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': predictions,\n",
        "        \"tweet\": textqueries,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(predictions) })\n",
        "    dev_df.to_csv('sexism_dev_predictions_task2.csv', index=False)\n",
        "    print(\"Evaluation TASK2 completed. Results saved to sexism_dev_predictions_task2.csv\")\n",
        "\n",
        "    #Computing the quality measure for the subtask 2\n",
        "    f1= f1_score(labelqueries, predictions, labels=None, average='macro')\n",
        "    print(f\"\\nMacro Average F1-Score Sexism: {f1}\\n\\n\")\n",
        "\n",
        "    idqueries= testData[0].tolist()\n",
        "    textqueries= testData[1].tolist()\n",
        "    test_predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt,  ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in[\"DIRECT\", \"REPORTED\", \"JUDGEMENTAL\"]:test_predictions.append(pred)\n",
        "        else: test_predictions.append(\"DIRECT\")\n",
        "        print(\"ID: \", idqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': test_predictions,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(test_predictions) })\n",
        "    submission_df.to_csv('sexism_predictions_task2.csv', index=False)\n",
        "    print(\"Prediction TASK2 completed. Results saved to sexism_predictions_task2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvHc-nLgX_DM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Zero-shot evaluation:   0%|          | 0/100 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(val_texts, desc=\u001b[33m\"\u001b[39m\u001b[33mZero-shot evaluation\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     17\u001b[39m     prompt = zero_shot_prompt(text)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     pred = \u001b[43mperform_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     zero_shot_preds.append(pred)\n\u001b[32m     21\u001b[39m zero_shot_f1 = f1_score(val_labels, zero_shot_preds, average=\u001b[33m'\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mperform_classification\u001b[39m\u001b[34m(model, tokenizer, prompt, ntokens, nseq, temp)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mperform_classification\u001b[39m(model, tokenizer, prompt, ntokens=\u001b[32m20\u001b[39m, nseq=\u001b[32m1\u001b[39m, temp=\u001b[32m0.7\u001b[39m):\n\u001b[32m     25\u001b[39m     inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Extract the category from the response\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3431\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3431\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    560\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    561\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         position_embeddings,\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:334\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m residual = hidden_states\n\u001b[32m    333\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    337\u001b[39m outputs = (hidden_states,)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:172\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     down_proj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "params=dict()\n",
        "incontext_zero_pipeline_task2(model, tokenizer, EnDevTask2, EnTestTask2, output_postprocessing_incontext_zero_s2, samples=100, **params)\n",
        "# val_ids = EnDevTask2[0].tolist()\n",
        "# val_texts = EnDevTask2[1].tolist()\n",
        "# val_labels = EnDevTask2[2].tolist()\n",
        "\n",
        "# # Create a list of dictionaries for sampling\n",
        "# data = [{'id': id_, 'text': text, 'label': label} for id_, text, label in zip(val_ids, val_texts, val_labels)]\n",
        "# n_samples = min(100, len(data))\n",
        "# val_subset = random.sample(data, n_samples)\n",
        "# val_texts = [item['text'] for item in val_subset]\n",
        "# val_labels = [item['label'] for item in val_subset]\n",
        "\n",
        "# # Evaluate zero-shot\n",
        "# zero_shot_preds = []\n",
        "# for text in tqdm(val_texts, desc=\"Zero-shot evaluation\"):\n",
        "#     prompt = zero_shot_prompt(text)\n",
        "#     pred = perform_classification(model, tokenizer, prompt)\n",
        "#     zero_shot_preds.append(pred)\n",
        "\n",
        "# zero_shot_f1 = f1_score(val_labels, zero_shot_preds, average='macro')\n",
        "# print(f\"Zero-shot F1-macro on {n_samples} samples: {zero_shot_f1:.4f}\")\n",
        "\n",
        "# # Evaluate few-shot\n",
        "# few_shot_preds = []\n",
        "# for text in tqdm(val_texts, desc=\"Few-shot evaluation\"):\n",
        "#     prompt = few_shot_prompt(text)\n",
        "#     pred = perform_classification(model, tokenizer, prompt)\n",
        "#     few_shot_preds.append(pred)\n",
        "\n",
        "# few_shot_f1 = f1_score(val_labels, few_shot_preds, average='macro')\n",
        "# print(f\"Few-shot F1-macro on {n_samples} samples: {few_shot_f1:.4f}\")\n",
        "\n",
        "# # Select best prompt\n",
        "# best_prompt = few_shot_prompt if few_shot_f1 > zero_shot_f1 else zero_shot_prompt\n",
        "# print(f\"Selected prompt: {'Few-shot' if few_shot_f1 > zero_shot_f1 else 'Zero-shot'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y74tkueEuShj"
      },
      "source": [
        "## Final Evaluation on Full Validation Set\n",
        "\n",
        "Using the best prompt, we evaluate on the entire validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvHc-nLgX_DM"
      },
      "outputs": [],
      "source": [
        "val_texts = [item['text'] for item in EnDevTask2]\n",
        "val_labels = [item['label'] for item in EnDevTask2]\n",
        "val_ids = [item['id'] for item in EnDevTask2]\n",
        "\n",
        "final_preds = []\n",
        "for text in tqdm(val_texts, desc=\"Final evaluation\"):\n",
        "    prompt = best_prompt(text)\n",
        "    pred = perform_classification(model, tokenizer, prompt)\n",
        "    final_preds.append(pred)\n",
        "\n",
        "final_f1 = f1_score(val_labels, final_preds, average='macro')\n",
        "print(f\"Final F1-macro on full validation set: {final_f1:.4f}\")\n",
        "\n",
        "# Display sample predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for id_, text, true, pred in zip(val_ids[:5], val_texts[:5], val_labels[:5], final_preds[:5]):\n",
        "    print(f\"ID: {id_} | Text: {text} | Ground Truth: {true} | Predicted: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y74tkueEuShj"
      },
      "source": [
        "## Report\n",
        "\n",
        "### Prompt Engineering Strategy\n",
        "- **Zero-shot**: Designed to be concise with clear instructions, specifying the task and categories. The output format was restricted to the category name to avoid ambiguity.\n",
        "- **Few-shot**: Included 6 examples, ensuring each category was represented. Examples were manually selected from the training set to be clear and diverse, covering typical cases of each category.\n",
        "\n",
        "### Example Selection\n",
        "- Examples were chosen to be unambiguous and representative of their respective categories.\n",
        "- Balanced representation was ensured by including two NON-SEXIST examples to reflect its prevalence in the dataset.\n",
        "\n",
        "### Evaluation Results\n",
        "- **Zero-shot F1-macro**: (Value from evaluation)\n",
        "- **Few-shot F1-macro**: (Value from evaluation)\n",
        "- **Final F1-macro**: (Value from full validation)\n",
        "\n",
        "### Challenges\n",
        "- **Computational Resources**: Loading and running LLaMA-2-7b on GPU was resource-intensive, requiring careful memory management.\n",
        "- **Model Output Consistency**: The model sometimes produced outputs not conforming to the expected category names, necessitating post-processing to map to valid categories.\n",
        "- **Category Imbalance**: The dataset had an imbalance, with NON-SEXIST being more frequent, which may have biased the model.\n",
        "\n",
        "### Visualizations\n",
        "(Include code for confusion matrix or bar plots of F1 scores per category if required)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

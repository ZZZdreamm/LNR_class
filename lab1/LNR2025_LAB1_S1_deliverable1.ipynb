{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7f5d08",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LAB1S1_p1. Tokenization with regular expresions</h1>\n",
    "\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5f16d",
   "metadata": {},
   "source": [
    "### Put your names here\n",
    "\n",
    "- Kacper Multan\n",
    "- Filip Polacik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac4aad-7e98-431a-a3e9-25a738b921a6",
   "metadata": {},
   "source": [
    "# Create tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791811a6-275c-4cf6-89e0-b1bc7723d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# 0\n",
    "word = r\"(\\w+\\-?(\\w+)?)\"\n",
    "\n",
    "\n",
    "def build_tokenizer():\n",
    "    # 1. Special symbols\n",
    "    special_symbols = r\"([()\\.\\,\\'\\\"\\?¿!¡…;:%])\"\n",
    "\n",
    "    # 2. Decimal numbers\n",
    "    decimal_number = r\"\\b\\d+[.,]?\\d+%?\"\n",
    "\n",
    "    # 3. Dates and times\n",
    "    date = r\"\\b\\d{1,2}[-/]\\d{1,2}(-\\d{4})?\\b\"\n",
    "    time = r\"\\b\\d{1,2}:\\d{2}\\s?[aA]\\.?\\s?[mM]\\.?[\\.\\b]\"\n",
    "\n",
    "    # 4. URLs\n",
    "    url = r\"(https?://[\\w./-|-~]+)\"\n",
    "\n",
    "    # 5. Email addresses\n",
    "    email = r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "    # 6. Mentions and hashtags\n",
    "    mention_hashtag = r\"[@#][\\w_-]+\"\n",
    "\n",
    "    # 7. Acronyms\n",
    "    acronym = r\"\\b(?:[A-Z]{2,}\\.[A-Z]{2,}\\.)|(?:[A-Z]\\.)+(?:[A-Z])?|\\.\\.\\.\"\n",
    "\n",
    "    # 8. Emoticons\n",
    "    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "    pattern_emoji = '(' + '|'.join(re.escape(u) for u in emojis) + ')'\n",
    "\n",
    "    # 9. Words\n",
    "    word = r\"(\\w+\\-?(\\w+)?)\"\n",
    "\n",
    "    pattern_all = r\"|\".join([\n",
    "        date,\n",
    "        time,\n",
    "        decimal_number,\n",
    "        url,\n",
    "        email,\n",
    "        mention_hashtag,\n",
    "        acronym,\n",
    "        pattern_emoji,\n",
    "        word,\n",
    "        special_symbols\n",
    "    ])\n",
    "\n",
    "    return re.compile(pattern_all, re.U)\n",
    "\n",
    "\n",
    "# Compile the tokenizer regex\n",
    "re_all = build_tokenizer()\n",
    "\n",
    "def tokenize(t, reg=re_all):\n",
    "    tokens = [t[match.start(): match.end()] for match in reg.finditer(t)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25e220-7e5a-4cf4-849c-3a75383b7b4f",
   "metadata": {},
   "source": [
    "# Test tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "37512ab5-7701-482c-b372-923a50949bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 00 ... ok!\n",
      "Sentence 01 ... ok!\n",
      "Sentence 02 ... ok!\n",
      "Sentence 03 ... ok!\n",
      "Sentence 04 ... ok!\n",
      "Sentence 05 ... ok!\n",
      "Sentence 06 ... ok!\n",
      "Sentence 07 ... ok!\n",
      "Sentence 08 ... ok!\n",
      "Sentence 09 ... ok!\n",
      "Sentence 10 ... ok!\n",
      "Sentence 11 ... ok!\n",
      "Sentence 12 ... ok!\n",
      "Sentence 13 ... ok!\n",
      "Sentence 14 ... ok!\n",
      "Sentence 15 ... ok!\n",
      "Sentence 16 ... ok!\n",
      "Sentence 17 ... ok!\n",
      "Sentence 18 ... ok!\n",
      "Sentence 19 ... ok!\n",
      "Sentence 20 ... ok!\n",
      "Sentence 21 ... ok!\n",
      "Sentence 22 ... ok!\n",
      "Sentence 23 ... ok!\n",
      "------------------------------\n",
      "Good work!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def test_tokenizer(tk_function, reference_file):\n",
    "    with open(reference_file, encoding=\"utf8\") as fh:\n",
    "        js = json.load(fh)\n",
    "    N, oks, errors = len(js), 0, 0\n",
    "    for i, dic in enumerate(js):\n",
    "        print(f\"Sentence {i:02} ... \", end=\"\")\n",
    "        text, ref = dic[\"text\"], dic[\"tokens\"]\n",
    "        tokens = tk_function(text)\n",
    "        if ref == tokens:\n",
    "            print(\"ok!\")\n",
    "        else:\n",
    "            errors += 1\n",
    "            missing = set(ref).difference(tokens)\n",
    "            leftover = set(tokens).difference(ref)\n",
    "            print(f\"errors: {list(leftover)} vs {list(missing)}\")\n",
    "    print(\"-\"*30)\n",
    "    if errors == 0:\n",
    "        print(\"Good work!\")\n",
    "    else:\n",
    "        print(f\"{errors} errors, keep working!\")\n",
    "\n",
    "\n",
    "# testing\n",
    "filename = \"input-tokenizer.json\"\n",
    "test_tokenizer(tokenize, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

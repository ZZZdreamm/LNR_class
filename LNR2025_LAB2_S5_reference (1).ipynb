{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24-_ZnyIuShY"
      },
      "source": [
        "<h1 align=\"center\">Lab 2:  Sexism Identification in Twitter</h1>\n",
        "<h2 align=\"center\">Session 5. Large Language Models: Prompting and In-context Learning</h2>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp_Qwn3cuShb"
      },
      "source": [
        "### Put your names here\n",
        "\n",
        "- (replace with one name)\n",
        "- (replace with another name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6YAOysPuShc"
      },
      "source": [
        "### CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nFj3DmItuShd"
      },
      "outputs": [],
      "source": [
        "COLAB = False # IF YOU USE GOOGLE COLAB -> COLAB = True\n",
        "PIP = True # IF YOU NEED INSTALL LIBRARIES -> PIP = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6MXclUuuShf"
      },
      "source": [
        "## Install some libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTA_gd1_OEz_",
        "outputId": "e969ae7f-63ef-4110-9e18-2f7e3dcd2823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: datasets in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (3.5.0)\n",
            "Requirement already satisfied: accelerate in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (69.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: evaluate in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
            "Requirement already satisfied: aiohttp in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: PyEvALL in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (0.1.78)\n",
            "Requirement already satisfied: jsbeautifier==1.14.9 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (1.14.9)\n",
            "Requirement already satisfied: jsonschema==4.23.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (4.23.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.3 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (2.2.3)\n",
            "Requirement already satisfied: setuptools==69.5.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (69.5.1)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from PyEvALL) (0.9.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsbeautifier==1.14.9->PyEvALL) (1.17.0)\n",
            "Requirement already satisfied: editorconfig>=0.12.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsbeautifier==1.14.9->PyEvALL) (0.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from jsonschema==4.23.0->PyEvALL) (0.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas==2.2.3->PyEvALL) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas==2.2.3->PyEvALL) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from pandas==2.2.3->PyEvALL) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema==4.23.0->PyEvALL) (4.13.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "if PIP:\n",
        "    !pip install transformers --upgrade\n",
        "    !pip install datasets accelerate\n",
        "    !pip install evaluate\n",
        "    !pip install -U PyEvALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VxMIGs_euShf"
      },
      "outputs": [],
      "source": [
        "#!pip install torch\n",
        "#!pip install numpy\n",
        "#!pip install pandas\n",
        "#!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RBb6LPE6uShg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "\n",
        "if False:\n",
        "    import numpy as np\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "    from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, accuracy_score\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import pandas as pd\n",
        "    from typing import List, Tuple\n",
        "    #from datasets import Dataset\n",
        "    import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6LJeq9TuShg"
      },
      "source": [
        "## Import readerEXIST2025 library, and read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfl9kuavv_mv",
        "outputId": "fe1e666e-38ae-4e57-9664-25571bc89672"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE2_dL-7uShh",
        "outputId": "69dbe7df-1991-4200-c4d8-77ca7f15e79c"
      },
      "outputs": [],
      "source": [
        "if COLAB is True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  base_path = \"/content/drive/MyDrive/LNR/\"\n",
        "else:\n",
        "  base_path = \"../\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PrbytJrIuShh"
      },
      "outputs": [],
      "source": [
        "library_path = os.path.join(base_path, \"Lab2-S1\")\n",
        "sys.path.append(library_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZpVKcZ64uShi",
        "outputId": "81a0e5b6-c253-461a-f5a8-6febc2bf72cd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from readerEXIST2025 import EXISTReader\n",
        "\n",
        "# path to the dataset, adapt this path wherever you have the dataset\n",
        "dataset_path = os.path.join(base_path, \"corpora/EXIST_2025_Dataset_V0.3/\")\n",
        "\n",
        "file_train = os.path.join(dataset_path, \"EXIST2025_training.json\")\n",
        "file_dev = os.path.join(dataset_path, \"EXIST2025_dev.json\")\n",
        "file_test = os.path.join(dataset_path, \"EXIST2025_test_clean.json\")\n",
        "\n",
        "\n",
        "reader_train = EXISTReader(file_train)\n",
        "reader_dev = EXISTReader(file_dev)\n",
        "reader_test = EXISTReader(file_test)\n",
        "\n",
        "\n",
        "EnTrainTask1, EnDevTask1, EnTestTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\"), reader_test.get(lang=\"EN\", subtask=\"1\")\n",
        "EnTrainTask2, EnDevTask2, EnTestTask2 = reader_train.get(lang=\"EN\", subtask=\"2\"), reader_dev.get(lang=\"EN\", subtask=\"2\"), reader_test.get(lang=\"EN\", subtask=\"2\")\n",
        "EnTrainTask3, EnDevTask3, EnTestTask3 = reader_train.get(lang=\"EN\", subtask=\"3\"), reader_dev.get(lang=\"EN\", subtask=\"3\"), reader_test.get(lang=\"EN\", subtask=\"3\")\n",
        "\n",
        "\n",
        "\n",
        "SpTrainTask1, SpDevTask1, SpTestTask1  = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\"), reader_test.get(lang=\"ES\", subtask=\"1\")\n",
        "SpTrainTask2, SpDevTask2, SpTestTask2  = reader_train.get(lang=\"ES\", subtask=\"2\"), reader_dev.get(lang=\"ES\", subtask=\"2\"), reader_test.get(lang=\"ES\", subtask=\"2\")\n",
        "SpTrainTask3, SpDevTask3, SpTestTask3  = reader_train.get(lang=\"ES\", subtask=\"3\"), reader_dev.get(lang=\"ES\", subtask=\"3\"), reader_test.get(lang=\"ES\", subtask=\"3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ob3lHgEuShi",
        "outputId": "97219100-7d12-4055-cb78-fe15ec658cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXIST2025_dev.json  EXIST2025_test_clean.json  EXIST2025_training.json\n"
          ]
        }
      ],
      "source": [
        "!ls ../corpora/EXIST_2025_Dataset_V0.3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y74tkueEuShj"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cbjBXOt9uShj"
      },
      "outputs": [],
      "source": [
        "class SexismDataset(Dataset):\n",
        "    def __init__(self, texts, labels, ids, tokenizer, max_len=128, pad=\"max_length\", trunc=True,rt='pt'):\n",
        "        self.texts = texts.tolist()\n",
        "        self.labels = labels\n",
        "        self.ids = ids\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.pad = pad\n",
        "        self.trunc = trunc\n",
        "        self.rt = rt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,padding=self.pad, truncation=self.trunc,\n",
        "            return_tensors=self.rt\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "            'id': torch.tensor(self.ids[idx], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dUXXawpruShj"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xDFrLJArQeoe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zzzdream/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "# Authenticate with your Hugging Face token\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "6aef6eac4f2b495985637863b4cc7184",
            "e313ea1b272c4b789820d3f180173146",
            "8fd072227b664aa9a2226689a726a182",
            "d9f25a6a4bd74c4e84f92188beb9db99",
            "f8c77d4c81a3437289629727ca9facba",
            "2fbda93dbc22498aa7484e62a7eec77b",
            "638e362a7d774ba89f7a4fdefc823812",
            "728715459bda4e4281f35b99b89dab68",
            "cb97765678d34bbdbabe52b77328d66d",
            "ceacdcc77bcf4e618846bbf09c65a4fc",
            "7b894e781fc345c18ca18994e7725c51",
            "ff4ec985d35a4ac5bd2a51627854faf0",
            "3b65273d7a234188a4b6f515996584d2",
            "38791fae63d14d6d9cb28c5acab83dc5",
            "25405681348b4a3ca9e339d908dbbd63",
            "5e794156492f4e5099b4581df75ef601",
            "26c69fc8f8ba45b188829a8329f7dc3c",
            "58ed7a5fbb904deca51d6b2fcdb066f9",
            "c9b6e69108344a9ebf7a529f1c832b0e",
            "3741d19cd69c4b13b3deb2ab63c98094",
            "04022b46c14947f18f99e6c60d465fd7",
            "90d9cb3afc5c4a3f84cd7a820cac179f",
            "113011f632ca43b5a21786b214c8e64c",
            "3a515bc8625d4ede94de204e94f06dfe",
            "42657bb518814208a7a31fa501e466de",
            "ccd6e306423a4c80af89cb3734e632e2",
            "063b0d61c1054b5db352e568314e3199",
            "86f3900468644fa2a3d72776a1008a99",
            "105a0029e5df40f6aea9a90e83b14212",
            "51ea0a07127442c7b85471406f39f642",
            "7fc30abe168142a69202c70bd739fdce",
            "3cf31f3acdc5442587be16be47225125",
            "5841290a6c5f48fe900a5fcdde7ed07f",
            "35bb332464d5410196bdcfc26d24a7af",
            "1f4cfb687cad4908b6ad172f885bdfba",
            "504d7548083e40babc631e0090d3338c",
            "53c807b2abac4162a5699d1c9a7847ae",
            "382dce0877754294aede9a7384530367",
            "46d762185c984592b0fc9e80c020d38a",
            "5029f042f0c54333a539e5aa23699c5d",
            "4e565d94d21e4c639fa2841a0b956255",
            "e28b81e8b9d74350bb020a99709a3842",
            "4bcff74fb8714ba4b570ba4bd919f076",
            "2fbbda15272349038767eac3366b64bc",
            "89b8a6623a484a5c8a203b7cf1a8184d",
            "0a70e5ecf6e44cd98677e6afa7bb8fd5",
            "db6d6cc647124009913b4204b04e344a",
            "c40e72abe56448e5b3a02a80e7f1f1a1",
            "e277108ffcbf4f0b8f606fd445dfacca",
            "bde47f9f0eec44b9b34576097986e985",
            "5a4ce71e103f42c7b4a0d185f51057d7",
            "6b89937737e34bc7824932cb86a313c9",
            "0fadd5be48694c1383e501718052a829",
            "b3226eb75a9f4b91a520d3eece3e7e42",
            "cfe783facc2244af83837a4b9dd06a98",
            "7b552548d9e94cc4bb5cf9cf10325dc5",
            "1873735b371c4ac495b45545e082c2d2",
            "2816daf71873449499b94d191049afb4",
            "41bf7f413fcf421ab8ae2805b5ce8c72",
            "558f58e89f974f10a61a849c797f2500",
            "53e1b2b2184e407cbbe35a2c72372689",
            "53bcd23cf2214f8baae5a79ad2ff8e0c",
            "88c94abf3842469480ad069cc8d74f5a",
            "0fb3b1edcbfe4fa1b3dee3fb5ab1325e",
            "d7096493c7c14c2ab5743d41419346bd",
            "d179f272ff79430980e0120943d3b26d"
          ]
        },
        "id": "YbFtXsbfN6T3",
        "outputId": "20cce0ac-ab33-4229-cc50-607a69064535"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 22.20it/s]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        "    token=HF_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "RJNp3UHMewT_",
        "outputId": "b1cfb581-2997-403d-f322-7d712b13a537"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m sexist_tweets = \u001b[33m\"\u001b[39m\u001b[33mi didntmean that hes not a whore hes an empowered woman who can  do whatever he wants with her body im so sorry\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput: Determine if the tweet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msexist_tweets\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m contains sexist language or not. Output:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43msimple_prompting_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36msimple_prompting_model\u001b[39m\u001b[34m(model, tokenizer, prompt)\u001b[39m\n\u001b[32m      2\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Generate responses on pretrained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Specify the new tokens that must be generated\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Determines the number of different sequences the model should generate\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Controls the randomness of the generated text. A higher temperature leads to more diverse and creative outputs, while a lower temperature results in more focused and deterministic outputs.\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    560\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    561\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         position_embeddings,\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:334\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m residual = hidden_states\n\u001b[32m    333\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    337\u001b[39m outputs = (hidden_states,)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:172\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def simple_prompting_model(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    # Generate responses on pretrained model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64, #Specify the new tokens that must be generated\n",
        "        num_return_sequences=1, # Determines the number of different sequences the model should generate\n",
        "        temperature=0.8 # Controls the randomness of the generated text. A higher temperature leads to more diverse and creative outputs, while a lower temperature results in more focused and deterministic outputs.\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "#######################################################################################################\n",
        "sexist_tweets = \"i didntmean that hes not a whore hes an empowered woman who can  do whatever he wants with her body im so sorry\"\n",
        "prompt = f\"Input: Determine if the tweet {sexist_tweets} contains sexist language or not. Output:\"\n",
        "simple_prompting_model(model, tokenizer, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dM01Kv1jVUov"
      },
      "outputs": [],
      "source": [
        "def perform_incontext_classification(model, tokenizer, prompt, ntokens=8, nseq=1, temp=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=ntokens,\n",
        "        num_return_sequences=nseq,\n",
        "        temperature=temp\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "NvHc-nLgX_DM",
        "outputId": "3903f8ad-8b99-4189-802d-a956c7bbaa04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID:  400001 Ground Truth: NO Predicted:  YES\n",
            "ID:  400002 Ground Truth: YES Predicted:  YES\n",
            "ID:  400003 Ground Truth: YES Predicted:  YES\n",
            "ID:  400004 Ground Truth: NO Predicted:  YES\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction TASK1 completed. Results saved to sexism_predictions_task1.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m params=\u001b[38;5;28mdict\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mincontext_zero_pipeline_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEnDevTask1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEnTestTask1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_postprocessing_incontext_zero_s1\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mincontext_zero_pipeline_task1\u001b[39m\u001b[34m(model, tokenizer, devData, testData, postprocess, **params)\u001b[39m\n\u001b[32m     51\u001b[39m filledPrompt=create_incontext_zero_prompt(task, query, context=\u001b[38;5;28;01mNone\u001b[39;00m, role=role, output=output)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#print(\"PROMPT >>> \", filledPrompt)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m response= \u001b[43mperform_incontext_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilledPrompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m#print(\"#\"*25, \"ANSWER\", \"#\"*25)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m#print(response)\u001b[39;00m\n\u001b[32m     56\u001b[39m pred=postprocess(response)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mperform_incontext_classification\u001b[39m\u001b[34m(model, tokenizer, prompt, ntokens, nseq, temp)\u001b[39m\n\u001b[32m      2\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    560\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    561\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         position_embeddings,\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:318\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:253\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    252\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    254\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    256\u001b[39m cos, sin = position_embeddings\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:171\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    173\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/hooks.py:342\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[32m    336\u001b[39m     module,\n\u001b[32m    337\u001b[39m     include_buffers=\u001b[38;5;28mself\u001b[39m.offload_buffers,\n\u001b[32m    338\u001b[39m     recurse=\u001b[38;5;28mself\u001b[39m.place_submodules,\n\u001b[32m    339\u001b[39m     remove_non_persistent=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    340\u001b[39m ):\n\u001b[32m    341\u001b[39m     fp16_statistics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name.replace(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSCB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weights_map.keys():\n\u001b[32m    344\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value.dtype == torch.int8:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/utils/offload.py:118\u001b[39m, in \u001b[36mPrefixedDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/studia/sem_6/natural_language_processing/.venv/lib/python3.12/site-packages/accelerate/utils/offload.py:171\u001b[39m, in \u001b[36mOffloadedWeightsLoader.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=device) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         tensor = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if failed to get_tensor on the device, such as bf16 on mps, try to load it on CPU first\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "def create_incontext_zero_prompt(task_description, query, context=None, role=None, output=None):\n",
        "    prompt = f\"\"\n",
        "    if role!=None:\n",
        "        prompt+= f\"You are an expert {role}.\\n\\n\"\n",
        "\n",
        "    prompt += f\"Task: {task_description}\\n\\n\"\n",
        "    if context!=None:\n",
        "         prompt+= f\"Context: {context}\\n\\n\"\n",
        "    if output != None:\n",
        "         prompt+= f\"Output Format: {output} \\n\\n\"\n",
        "    # Add query\n",
        "    prompt+=f\"Input: {query}\\n\"\n",
        "    prompt+=f\"Output: \"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def output_postprocessing_incontext_zero_s1(output):\n",
        "    outputp=output.rsplit(\"Output: \", 1)[-1].strip()\n",
        "    #print(outputp)\n",
        "    for line in outputp.split('\\n'):\n",
        "        line=line.strip()\n",
        "        if line!=\"\":\n",
        "             if line.upper().startswith(\"YES\"): return \"YES\"\n",
        "             if line.upper().startswith(\"NO\"): return \"NO\"\n",
        "    return \"UNK\"\n",
        "\n",
        "def incontext_zero_pipeline_task1(model, tokenizer, devData, testData, postprocess, **params):\n",
        "\n",
        "    role= \"in social psychology and linguistics with vast experience analyzing social media content and discriminative and harmful language\"\n",
        "    task =\"\"\"Sexist identification task is a binaty text classification task which aim at determining\n",
        "    whether or not a given tweet expresses ideas related to sexism in any of the three forms: it is sexist itself,\n",
        "    it describes a sexist situation in which discrimination towards women occurs, or criticizes a sexist behaviour.\n",
        "    The tweet is sexist (YES) or describes or criticizes a sexist situation. Not sexist. The tweet is not sexist (NO),\n",
        "    nor it describes or criticizes a sexist situation.\"\"\"\n",
        "    output= \"The output must be YES/NO.\"\n",
        "    context=None\n",
        "\n",
        "    ntokens=params.get(\"max_new_tokens\", 8)\n",
        "    nseq=params.get(\"num_return_sequences\", 1)\n",
        "    temp=params.get(\"temperature\", 0.7)\n",
        "\n",
        "    idqueries= devData[0].tolist()\n",
        "    textqueries= devData[1].tolist()\n",
        "    labelqueries=devData[2].tolist()\n",
        "\n",
        "    predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt, ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocess(response)\n",
        "        if pred in [\"YES\", \"NO\"]:\n",
        "            predictions.append(pred)\n",
        "        else:\n",
        "            predictions.append(\"NO\")\n",
        "        print(\"ID: \", idqueries[i], \"Ground Truth:\", labelqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create dev output DataFrame\n",
        "    dev_df = pd.DataFrame({ 'id': idqueries,  'label': predictions, \"tweet\": textqueries, \"test_case\": [\"EXIST2025\"]*len(predictions) })\n",
        "    dev_df.to_csv('sexism_dev_predictions_task1.csv', index=False)\n",
        "    print(\"Prediction TASK1 completed. Results saved to sexism_dev_predictions_task1.csv\")\n",
        "\n",
        "    #Computing the quality measure for the subtask 1\n",
        "    f1= f1_score(labelqueries, predictions, labels=None, pos_label=\"YES\", average='binary')\n",
        "    print(f\"\\nF1-Score Sexism: {f1}\\n\\n\")\n",
        "\n",
        "    idqueries= testData[0].tolist()\n",
        "    textqueries= testData[1].tolist()\n",
        "    test_predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt,  ntokens, nseq, temp)\n",
        "        pred=postprocess(response)\n",
        "        if pred in [\"YES\", \"NO\"]:test_predictions.append(pred)\n",
        "        else: test_predictions.append(\"NO\")\n",
        "        print(\"ID: \", idqueries[i],  \"Predicted: \", pred)\n",
        "        # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({'id': idqueries, 'label': test_predictions, \"test_case\": [\"EXIST2025\"]*len(test_predictions) })\n",
        "    submission_df.to_csv('sexism_predictions_task1.csv', index=False)\n",
        "    print(\"Prediction TASK1 completed. Results saved to sexism_predictions_task1.csv\")\n",
        "\n",
        "params=dict()\n",
        "incontext_zero_pipeline_task1(model, tokenizer, EnDevTask1, EnTestTask1, output_postprocessing_incontext_zero_s1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pry2by-V7hyT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "\n",
        "def output_postprocessing_incontext_zero_s2(output):\n",
        "    outputp=output.rsplit(\"Output: \", 1)[-1].strip()\n",
        "    for line in outputp.split('\\n'):\n",
        "        line=line.strip()\n",
        "        if line!=\"\":\n",
        "             if line.upper().startswith(\"DIRECT\"): return \"DIRECT\"\n",
        "             if line.upper().startswith(\"REPORTED\"): return \"REPORTED\"\n",
        "             if line.upper().startswith(\"JUDGEMENTAL\"): return \"JUDGEMENTAL\"\n",
        "    return \"UNK\"\n",
        "\n",
        "def create_incontext_zero_prompt(task_description, query, context=None, role=None, output=None):\n",
        "    prompt = f\"\"\n",
        "    if role!=None:\n",
        "        prompt+= f\"You are an expert {role}.\\n\\n\"\n",
        "\n",
        "    prompt += f\"Task: {task_description}\\n\\n\"\n",
        "    if context!=None:\n",
        "         prompt+= f\"Context: {context}\\n\\n\"\n",
        "    if output != None:\n",
        "         prompt+= f\"Output Format: {output} \\n\\n\"\n",
        "    # Add query\n",
        "    prompt+=f\"Input: {query}\\n\"\n",
        "    prompt+=f\"Output: \"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def incontext_zero_pipeline_task2(model, tokenizer, devData, testData, postprocessing, **params):\n",
        "\n",
        "    role= \"in social psychology and linguistics with vast experience analyzing social media content and discriminative and harmful language\"\n",
        "    task =\"\"\"Sexism Source Intention in tweets is a three class classification task aims to categorize\n",
        "    the sexist tweets according to the intention of the author. This distinction allow us to differentiate\n",
        "    sexism that is actually taking place online from sexism which is being suffered by women in other situations\n",
        "    but that is being reported in social networks with the aim of complaining and fighting against sexism.\n",
        "    The task include the following ternary classification of tweets.\n",
        "\n",
        "    REPORTED: the intention is to report and share a sexist situation suffered by a woman or women.\n",
        "    JUDGEMENTAL the intention is to condemn and critizise sexist situations or behaviours.\n",
        "    DIRECT: The intention is to write a message that is sexist by itself or incites to be sexist.\n",
        "     \"\"\"\n",
        "    output= \"The output is a single label: REPORTED/JUDGEMENTAL/DIRECT.\"\n",
        "    context=None\n",
        "\n",
        "    ntokens=params.get(\"max_new_tokens\", 8)\n",
        "    nseq=params.get(\"num_return_sequences\", 1)\n",
        "    temp=params.get(\"temperature\", 0.7)\n",
        "\n",
        "    idqueries= devData[0].tolist()\n",
        "    textqueries= devData[1].tolist()\n",
        "    labelqueries=devData[2].tolist()\n",
        "\n",
        "    predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt, ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in [\"DIRECT\", \"REPORTED\", \"JUDGEMENTAL\"]: predictions.append(pred)\n",
        "        else: predictions.append(\"DIRECT\")\n",
        "        print(\"ID: \", idqueries[i], \"Ground Truth:\", labelqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create dev output DataFrame\n",
        "    dev_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': predictions,\n",
        "        \"tweet\": textqueries,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(predictions) })\n",
        "    dev_df.to_csv('sexism_dev_predictions_task2.csv', index=False)\n",
        "    print(\"Evaluation TASK2 completed. Results saved to sexism_dev_predictions_task2.csv\")\n",
        "\n",
        "    #Computing the quality measure for the subtask 2\n",
        "    f1= f1_score(labelqueries, predictions, labels=None, average='macro')\n",
        "    print(f\"\\nMacro Average F1-Score Sexism: {f1}\\n\\n\")\n",
        "\n",
        "    idqueries= testData[0].tolist()\n",
        "    textqueries= testData[1].tolist()\n",
        "    test_predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt,  ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in[\"DIRECT\", \"REPORTED\", \"JUDGEMENTAL\"]:test_predictions.append(pred)\n",
        "        else: test_predictions.append(\"DIRECT\")\n",
        "        print(\"ID: \", idqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': test_predictions,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(test_predictions) })\n",
        "    submission_df.to_csv('sexism_predictions_task2.csv', index=False)\n",
        "    print(\"Prediction TASK2 completed. Results saved to sexism_predictions_task2.csv\")\n",
        "\n",
        "params=dict()\n",
        "incontext_zero_pipeline_task2(model, tokenizer, EnDevTask2, EnTestTask2, output_postprocessing_incontext_zero_s2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi444zQCA3Oi"
      },
      "outputs": [],
      "source": [
        "import os, sys, tempfile, pandas as pd\n",
        "import numpy as np\n",
        "from pyevall.evaluation import PyEvALLEvaluation\n",
        "from pyevall.metrics.metricfactory import MetricFactory\n",
        "from pyevall.reports.reports import PyEvALLReport\n",
        "from pyevall.utils.utils import PyEvALLUtils\n",
        "\n",
        "def ICMWrapper(pred, labels, multi=False,ids=None):\n",
        "    test = PyEvALLEvaluation()\n",
        "    metrics=[MetricFactory.ICM.value]\n",
        "    params= dict()\n",
        "    fillLabel=None\n",
        "    if multi:\n",
        "        params[PyEvALLUtils.PARAM_REPORT]=\"embedded\"\n",
        "        hierarchy={\"True\":['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE'],\n",
        "        \"False\":[]}\n",
        "        params[PyEvALLUtils.PARAM_HIERARCHY]=hierarchy\n",
        "        fillLabel = lambda x: [\"False\"] if len(x)== 0 else x\n",
        "    else:\n",
        "        params[PyEvALLUtils.PARAM_REPORT]=\"simple\"\n",
        "        fillLabel = lambda x: str(x)\n",
        "\n",
        "\n",
        "    truth_name, predict_name=None, None\n",
        "    if ids is None:\n",
        "        ids=list(range(len(labels)))\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as truth:\n",
        "        truth_name=truth.name\n",
        "        truth_df=pd.DataFrame({'test_case': ['EXIST2025']*len(labels),\n",
        "                        'id': [str(x) for x in ids],\n",
        "                        'value': [fillLabel(x) for x in labels]})\n",
        "        if multi==True:\n",
        "            truth_df=truth_df.astype('object')\n",
        "        truth.write(truth_df.to_json(orient=\"records\"))\n",
        "\n",
        "    with  tempfile.NamedTemporaryFile(mode='w', delete=False) as predict:\n",
        "        predict_name=predict.name\n",
        "        predict_df=pd.DataFrame({'test_case': ['EXIST2025']*len(pred),\n",
        "                        'id': [str(x) for x in ids],\n",
        "                        'value': [fillLabel(x) for x in pred]})\n",
        "        if multi==True:\n",
        "            predict_df=predict_df.astype('object')\n",
        "        predict.write(predict_df.to_json(orient=\"records\"))\n",
        "\n",
        "    report = test.evaluate(predict_name, truth_name, metrics, **params)\n",
        "    os.unlink(truth_name)\n",
        "    os.unlink(predict_name)\n",
        "\n",
        "    icm = None\n",
        "    if 'metrics' in report.report:\n",
        "        if 'ICM' in report.report[\"metrics\"]: icm=float(report.report[\"metrics\"]['ICM'][\"results\"][\"average_per_test_case\"])\n",
        "    return icm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z41X-MHJ8lAv"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "def output_postprocessing_incontext_zero_s3(output):\n",
        "    cLabels=['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE']\n",
        "    labels=[]\n",
        "    outputp=output.rsplit(\"Output: \", 1)[-1].strip()\n",
        "    #print(outputp)\n",
        "    for line in outputp.split('\\n'):\n",
        "        line=line.strip()\n",
        "        if line!=\"\":\n",
        "             #print(line)\n",
        "             if line.upper().startswith(\"[\"):\n",
        "                try:\n",
        "                    labels = [x.upper() for  x in ast.literal_eval(line) if x.upper() in cLabels]\n",
        "                    return labels\n",
        "                except:\n",
        "                    labels=[]\n",
        "    return labels\n",
        "\n",
        "def create_incontext_zero_prompt(task_description, query, context=None, role=None, output=None):\n",
        "    prompt = f\"\"\n",
        "    if role!=None:\n",
        "        prompt+= f\"You are an expert {role}.\\n\\n\"\n",
        "\n",
        "    prompt += f\"Task: {task_description}\\n\\n\"\n",
        "    if context!=None:\n",
        "         prompt+= f\"Context: {context}\\n\\n\"\n",
        "    if output != None:\n",
        "         prompt+= f\"Output Format: {output} \\n\\n\"\n",
        "    # Add query\n",
        "    prompt+=f\"Input: {query}\\n\"\n",
        "    prompt+=f\"Output: \"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "\n",
        "def incontext_zero_pipeline_task3(model, tokenizer, devData, testData, postprocessing, **params):\n",
        "\n",
        "    role= \"in social psychology and linguistics with vast experience analyzing social media content and discriminative and harmful language\"\n",
        "    task =\"\"\" Many facets of a woman’s life may be the focus of sexist attitudes including domestic and parenting roles,  career opportunities, sexual image, and life expectations, to name a few. According to this, each sexist tweet must be assigned one or more of the five categories.\n",
        "\n",
        "    IDEOLOGICAL-AND-INEQUALITY: it includes messages that discredit the feminist movement. It also includes messages that reject inequality between men and women, or present men as victims of gender-based oppression.\n",
        "\n",
        "    STEREOTYPING-DOMINANCE: it includes messages that express false ideas about women that suggest they are more suitable or inappropriate for certain tasks, and somehow inferior to men.\n",
        "\n",
        "    OBJECTIFICATION: it includes messages where women are presented as objects apart from their dignity and personal aspects. We also include messages that assume or describe certain physical ºqualities that women must have in order to fulfill traditional gender roles\n",
        "\n",
        "    SEXUAL-VIOLENCE: it includes messages where sexual suggestions, requests or harassment of a sexual nature (rape or sexual assault) are made.\n",
        "\n",
        "    MISOGYNY-NON-SEXUAL-VIOLENCE: it includes expressions of hatred and violence towards women.\"\"\"\n",
        "\n",
        "    output= \"The output must be a list of the assigned label. Possible Labels: ['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE']\"\n",
        "    context=None\n",
        "\n",
        "    ntokens=params.get(\"max_new_tokens\", 32)\n",
        "    nseq=params.get(\"num_return_sequences\", 1)\n",
        "    temp=params.get(\"temperature\", 0.7)\n",
        "\n",
        "    idqueries= devData[0].tolist()\n",
        "    textqueries= devData[1].tolist()\n",
        "    labelqueries=devData[2].tolist()\n",
        "\n",
        "    predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt, ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        predictions.append(pred)\n",
        "        print(\"ID: \", idqueries[i], \"Ground Truth:\", labelqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create dev output DataFrame\n",
        "    dev_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': predictions,\n",
        "        \"tweet\": textqueries,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(predictions) })\n",
        "    dev_df.to_csv('sexism_dev_predictions_task3.csv', index=False)\n",
        "    print(\"Evaluation TASK3 completed. Results saved to sexism_dev_predictions_task3.csv\")\n",
        "\n",
        "    #Computing the quality measure for the subtask 1\n",
        "    icm= ICMWrapper(predictions, labelqueries, multi=True)\n",
        "    print(f\"\\nICM : {icm}\\n\\n\")\n",
        "\n",
        "    idqueries= testData[0].tolist()\n",
        "    textqueries= testData[1].tolist()\n",
        "    test_predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_zero_prompt(task, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt,  ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        test_predictions.append(pred)\n",
        "        print(\"ID: \", idqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "        # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': test_predictions,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(test_predictions) })\n",
        "    submission_df.to_csv('sexism_predictions_task3.csv', index=False)\n",
        "    print(\"Prediction TASK3 completed. Results saved to sexism_predictions_task3.csv\")\n",
        "\n",
        "params=dict()\n",
        "incontext_zero_pipeline_task3(model, tokenizer, EnDevTask3, EnTestTask3, output_postprocessing_incontext_zero_s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ax867ibEyv1"
      },
      "outputs": [],
      "source": [
        "def create_incontext_few_prompt(task_description, exemplars, query, context=None, role=None, output=None):\n",
        "    prompt = f\"\"\n",
        "    if role!=None:\n",
        "        prompt+= f\"You are an expert {role}.\\n\\n\"\n",
        "\n",
        "    prompt += f\"Task: {task_description}\\n\\n\"\n",
        "    if context!=None:\n",
        "         prompt+= f\"Context: {context}\\n\\n\"\n",
        "\n",
        "    prompt += \"Examples in Input and Output format:\\n\\n\"\n",
        "    # Add exemplars\n",
        "    for exemplar in exemplars:\n",
        "        prompt += f\"Input: {exemplar['input']}\\n\"\n",
        "        prompt += f\"Output: {exemplar['output']}\\n\\n\"\n",
        "\n",
        "    if output != None:\n",
        "         prompt+= f\"Output Format: {output} \\n\\n\"\n",
        "    # Add query\n",
        "    prompt+=f\"Input: {query}\\nOutput:\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtWgFtI5OjNL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def output_postprocessing_incontext_few_s1(output):\n",
        "    outputp=output.rsplit(\"\\nOutput:\", 1)[-1].strip()\n",
        "    #print(outputp)\n",
        "    for line in outputp.split('\\n'):\n",
        "        line=line.strip()\n",
        "        if line!=\"\":\n",
        "             if line.upper().startswith(\"YES\"): return \"YES\"\n",
        "             if line.upper().startswith(\"NO\"): return \"NO\"\n",
        "    return \"UNK\"\n",
        "\n",
        "\n",
        "def stratified_sample(df, label_column, sample_size=None, sample_frac=None):\n",
        "    \"\"\"\n",
        "    Stratified sampling of a DataFrame based on a label column.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to sample.\n",
        "        label_column (str): The name of the column to stratify by.\n",
        "        sample_size (int, optional): The desired sample size per stratum.\n",
        "        sample_frac (float, optional): The fraction of samples to take per stratum.\n",
        "            Either `sample_size` or `sample_frac` must be provided.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The stratified sample of the DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If neither `sample_size` nor `sample_frac` is provided.\n",
        "    \"\"\"\n",
        "\n",
        "    if sample_size is None and sample_frac is None:\n",
        "        raise ValueError(\"Either `sample_size` or `sample_frac` must be provided.\")\n",
        "    # Group the DataFrame by the label column\n",
        "    grouped = df.groupby(label_column)\n",
        "    # Determine the sample size or fraction for each group\n",
        "    if sample_size is not None:\n",
        "        sample_sizes = {label: sample_size for label in grouped.groups}\n",
        "    else:\n",
        "        sample_fracs = {label: sample_frac for label in grouped.groups}\n",
        "    # Sample each group\n",
        "    samples = []\n",
        "    for label, group in grouped:\n",
        "        if sample_size is not None:\n",
        "            sample = group.sample(n=sample_sizes[label], random_state=12324)\n",
        "        else:\n",
        "            sample = group.sample(frac=sample_fracs[label], random_state=12344)\n",
        "        samples.append(sample)\n",
        "\n",
        "    # Concatenate the samples\n",
        "    stratified_sample = pd.concat(samples, ignore_index=True)\n",
        "    return stratified_sample\n",
        "\n",
        "\n",
        "#This function aims at selecting the examples used in the in-context learning\n",
        "# using few-shot. This is a naive a papproach to inform the propmt with examples in the trainig dataset\n",
        "def sampling_few_instances(trainData, nexamples):\n",
        "    idqueries= trainData[0].tolist()\n",
        "    textqueries= trainData[1].tolist()\n",
        "    labelqueries=trainData[2].tolist()\n",
        "    df = pd.DataFrame({'id': idqueries, 'tweet': textqueries, 'label': labelqueries})\n",
        "    samples=stratified_sample(df, 'label', nexamples)\n",
        "    examples=[]\n",
        "    for i, row in samples.iterrows():\n",
        "        examples.append({'input': row['tweet'], 'output': row['label']})\n",
        "    return examples\n",
        "\n",
        "\n",
        "\n",
        "def incontext_few_pipeline_task1(model, tokenizer, trainData, devData, testData, postprocessing,**params):\n",
        "\n",
        "    role= \"in social psychology and linguistics with vast experience analyzing social media content and discriminative and harmful language\"\n",
        "    task =\"\"\"Sexist identification task is a binaty text classification task which aim at determining\n",
        "    whether or not a given tweet expresses ideas related to sexism in any of the three forms: it is sexist itself,\n",
        "    it describes a sexist situation in which discrimination towards women occurs, or criticizes a sexist behaviour.\n",
        "    The tweet is sexist (YES) or describes or criticizes a sexist situation. Not sexist. The tweet is not sexist (NO),\n",
        "    nor it describes or criticizes a sexist situation.\"\"\"\n",
        "    output= \"The output must be YES/NO.\"\n",
        "    context=None\n",
        "\n",
        "    ntokens=params.get(\"max_new_tokens\", 8)\n",
        "    nseq=params.get(\"num_return_sequences\", 1)\n",
        "    temp=params.get(\"temperature\", 0.7)\n",
        "\n",
        "    idqueries= devData[0].tolist()\n",
        "    textqueries= devData[1].tolist()\n",
        "    labelqueries=devData[2].tolist()\n",
        "\n",
        "    exemplars = sampling_few_instances(trainData, 3)\n",
        "\n",
        "    predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_few_prompt(task, exemplars, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt, ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in [\"YES\", \"NO\"]:predictions.append(pred)\n",
        "        else: predictions.append(\"NO\")\n",
        "        print(\"ID: \", idqueries[i], \"Ground Truth:\", labelqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create dev output DataFrame\n",
        "    dev_df = pd.DataFrame({'id': idqueries,  'label': predictions, \"tweet\": textqueries, \"test_case\": [\"EXIST2025\"]*len(predictions) })\n",
        "    dev_df.to_csv('sexism_dev_predictions_task1_few.csv', index=False)\n",
        "    print(\"Prediction TASK1 completed. Results saved to sexism_dev_predictions_task1_few.csv\")\n",
        "\n",
        "    #Computing the quality measure for the subtask 1\n",
        "    f1= f1_score(labelqueries, predictions, labels=None, pos_label=\"YES\", average='binary')\n",
        "    print(f\"\\nF1-Score Sexism: {f1}\\n\\n\")\n",
        "\n",
        "    idqueries= testData[0].tolist()\n",
        "    textqueries= testData[1].tolist()\n",
        "    test_predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_few_prompt(task, exemplars, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt,  ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in [\"YES\", \"NO\"]:test_predictions.append(pred)\n",
        "        else: test_predictions.append(\"NO\")\n",
        "        print(\"ID: \", idqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "        # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': test_predictions,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(test_predictions) })\n",
        "    submission_df.to_csv('sexism_predictions_task1_few.csv', index=False)\n",
        "    print(\"Prediction TASK1 completed. Results saved to sexism_predictions_task1_few.csv\")\n",
        "\n",
        "params = dict()\n",
        "incontext_few_pipeline_task1(model, tokenizer,EnTrainTask1,  EnDevTask1, EnTestTask1, output_postprocessing_incontext_few_s1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG6yEe1xq3rY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def output_postprocessing_incontext_few_s2(output):\n",
        "    outputp=output.rsplit(\"\\nOutput:\", 1)[-1].strip()\n",
        "    for line in outputp.split('\\n'):\n",
        "        line=line.strip()\n",
        "        if line!=\"\":\n",
        "             if line.upper().startswith(\"DIRECT\"): return \"DIRECT\"\n",
        "             if line.upper().startswith(\"REPORTED\"): return \"REPORTED\"\n",
        "             if line.upper().startswith(\"JUDGEMENTAL\"): return \"JUDGEMENTAL\"\n",
        "    return \"UNK\"\n",
        "\n",
        "\n",
        "def stratified_sample(df, label_column, sample_size=None, sample_frac=None):\n",
        "    \"\"\"\n",
        "    Stratified sampling of a DataFrame based on a label column.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to sample.\n",
        "        label_column (str): The name of the column to stratify by.\n",
        "        sample_size (int, optional): The desired sample size per stratum.\n",
        "        sample_frac (float, optional): The fraction of samples to take per stratum.\n",
        "            Either `sample_size` or `sample_frac` must be provided.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The stratified sample of the DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If neither `sample_size` nor `sample_frac` is provided.\n",
        "    \"\"\"\n",
        "\n",
        "    if sample_size is None and sample_frac is None:\n",
        "        raise ValueError(\"Either `sample_size` or `sample_frac` must be provided.\")\n",
        "    # Group the DataFrame by the label column\n",
        "    grouped = df.groupby(label_column)\n",
        "    # Determine the sample size or fraction for each group\n",
        "    if sample_size is not None:\n",
        "        sample_sizes = {label: sample_size for label in grouped.groups}\n",
        "    else:\n",
        "        sample_fracs = {label: sample_frac for label in grouped.groups}\n",
        "    # Sample each group\n",
        "    samples = []\n",
        "    for label, group in grouped:\n",
        "        if sample_size is not None:\n",
        "            sample = group.sample(n=sample_sizes[label], random_state=1234)\n",
        "        else:\n",
        "            sample = group.sample(frac=sample_fracs[label], random_state=1234)\n",
        "        samples.append(sample)\n",
        "\n",
        "    # Concatenate the samples\n",
        "    stratified_sample = pd.concat(samples, ignore_index=True)\n",
        "    return stratified_sample\n",
        "\n",
        "\n",
        "#This function aims at selecting the examples used in the in-context learning\n",
        "# using few-shot. This is a naive a approach to inform the prompt with examples in the training dataset\n",
        "def sampling_few_instances(trainData, nexamples):\n",
        "    idqueries= trainData[0].tolist()\n",
        "    textqueries= trainData[1].tolist()\n",
        "    labelqueries=trainData[2].tolist()\n",
        "    df = pd.DataFrame({'id': idqueries, 'tweet': textqueries, 'label': labelqueries})\n",
        "    samples=stratified_sample(df, 'label', nexamples)\n",
        "    examples=[]\n",
        "    for i, row in samples.iterrows():\n",
        "        examples.append({'input': row['tweet'], 'output': row['label']})\n",
        "    return examples\n",
        "\n",
        "\n",
        "\n",
        "def incontext_few_pipeline_task2(model, tokenizer, trainData, devData, testData, postprocessing, **params):\n",
        "    role= \"in social psychology and linguistics with vast experience analyzing social media content and discriminative and harmful language\"\n",
        "    task =\"\"\"Sexism Source Intention in tweets is a three class classification task aims to categorize\n",
        "    the sexist tweets according to the intention of the author. This distinction allow us to differentiate\n",
        "    sexism that is actually taking place online from sexism which is being suffered by women in other situations\n",
        "    but that is being reported in social networks with the aim of complaining and fighting against sexism.\n",
        "    The task include the following ternary classification of tweets.\n",
        "\n",
        "    REPORTED: the intention is to report and share a sexist situation suffered by a woman or women.\n",
        "    JUDGEMENTAL the intention is to condemn and critizise sexist situations or behaviours.\n",
        "    DIRECT: The intention is to write a message that is sexist by itself or incites to be sexist.\n",
        "     \"\"\"\n",
        "    output= \"The output is a single label: REPORTED/JUDGEMENTAL/DIRECT.\"\n",
        "    context=None\n",
        "\n",
        "    ntokens=params.get(\"max_new_tokens\", 8)\n",
        "    nseq=params.get(\"num_return_sequences\", 1)\n",
        "    temp=params.get(\"temperature\", 0.7)\n",
        "\n",
        "    idqueries= devData[0].tolist()\n",
        "    textqueries= devData[1].tolist()\n",
        "    labelqueries=devData[2].tolist()\n",
        "\n",
        "    exemplars = sampling_few_instances(trainData, 2)\n",
        "\n",
        "    predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_few_prompt(task, exemplars, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt, ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in [\"DIRECT\", \"REPORTED\", \"JUDGEMENTAL\"]:predictions.append(pred)\n",
        "        else: predictions.append(\"DIRECT\")\n",
        "        print(\"ID: \", idqueries[i],\"Ground Truth:\", labelqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create dev output DataFrame\n",
        "    dev_df = pd.DataFrame({'id': idqueries,  'label': predictions, \"tweet\": textqueries, \"test_case\": [\"EXIST2025\"]*len(predictions) })\n",
        "    dev_df.to_csv('sexism_dev_predictions_task2_few.csv', index=False)\n",
        "    print(\"Prediction TASK2 completed. Results saved to sexism_dev_predictions_task2_few.csv\")\n",
        "\n",
        "    #Computing the quality measure for the subtask 1\n",
        "    f1= f1_score(labelqueries, predictions, average='macro')\n",
        "    print(f\"\\nMacro average F1-Score Sexism: {f1}\\n\\n\")\n",
        "\n",
        "    idqueries= testData[0].tolist()\n",
        "    textqueries= testData[1].tolist()\n",
        "    test_predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_few_prompt(task, exemplars, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt,  ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        if pred in [\"DIRECT\", \"REPORTED\", \"JUDGEMENTAL\"]:test_predictions.append(pred)\n",
        "        else: test_predictions.append(\"DIRECT\")\n",
        "        print(\"ID: \", idqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "        # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': test_predictions,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(test_predictions) })\n",
        "    submission_df.to_csv('sexism_predictions_task2_few.csv', index=False)\n",
        "    print(\"Prediction TASK2 completed. Results saved to sexism_predictions_task2_few.csv\")\n",
        "\n",
        "params=dict()\n",
        "incontext_few_pipeline_task2(model, tokenizer,EnTrainTask2,  EnDevTask2, EnTestTask2, output_postprocessing_incontext_few_s2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EStnJXc00wop",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import ast\n",
        "def output_postprocessing_incontext_zero_s3(output):\n",
        "    cLabels=['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE']\n",
        "    labels=[]\n",
        "    outputp=output.rsplit(\"\\nOutput:\", 1)[-1].strip()\n",
        "    #print(outputp)\n",
        "    for line in outputp.split('\\n'):\n",
        "        line=line.strip()\n",
        "        if line!=\"\":\n",
        "             #print(line)\n",
        "             if line.upper().startswith(\"[\"):\n",
        "                try:\n",
        "                    labels = [x.upper() for  x in ast.literal_eval(line) if x.upper() in cLabels]\n",
        "                    return labels\n",
        "                except:\n",
        "                    labels=[]\n",
        "    return labels\n",
        "\n",
        "\n",
        "#This function aims at selecting the examples used in the in-context learning\n",
        "# using few-shot. This is a naive a papproach to inform the propmt with examples in the trainig dataset\n",
        "def sampling_few_instances_multilabel(trainData, nexamples):\n",
        "    idqueries= trainData[0].tolist()\n",
        "    textqueries= trainData[1].tolist()\n",
        "    labelqueries=trainData[2].tolist()\n",
        "    df = pd.DataFrame({'id': idqueries, 'tweet': textqueries, 'label': labelqueries})\n",
        "    samples= df.sample(n=min(nexamples, len(df)), random_state=1234)\n",
        "    examples=[]\n",
        "    for i, row in samples.iterrows():\n",
        "        examples.append({'input': row['tweet'], 'output': row['label']})\n",
        "    return examples\n",
        "\n",
        "\n",
        "\n",
        "def incontext_few_pipeline_task3(model, tokenizer, trainData, devData, testData, postprocessing, **params):\n",
        "\n",
        "    role= \"in social psychology and linguistics with vast experience analyzing social media content and discriminative and harmful language\"\n",
        "    task =\"\"\" Many facets of a woman’s life may be the focus of sexist attitudes including domestic and parenting roles,  career opportunities, sexual image, and life expectations, to name a few. According to this, each sexist tweet must be assigned one or more of the five categories.\n",
        "\n",
        "    IDEOLOGICAL-AND-INEQUALITY: it includes messages that discredit the feminist movement. It also includes messages that reject inequality between men and women, or present men as victims of gender-based oppression.\n",
        "\n",
        "    STEREOTYPING-DOMINANCE: it includes messages that express false ideas about women that suggest they are more suitable or inappropriate for certain tasks, and somehow inferior to men.\n",
        "\n",
        "    OBJECTIFICATION: it includes messages where women are presented as objects apart from their dignity and personal aspects. We also include messages that assume or describe certain physical ºqualities that women must have in order to fulfill traditional gender roles\n",
        "\n",
        "    SEXUAL-VIOLENCE: it includes messages where sexual suggestions, requests or harassment of a sexual nature (rape or sexual assault) are made.\n",
        "\n",
        "    MISOGYNY-NON-SEXUAL-VIOLENCE: it includes expressions of hatred and violence towards women.\"\"\"\n",
        "\n",
        "    output= \"The output must be a list of the assigned label. Possible Labels: ['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE']\"\n",
        "    context=None\n",
        "\n",
        "    ntokens=params.get(\"max_new_tokens\", 48)\n",
        "    nseq=params.get(\"num_return_sequences\", 1)\n",
        "    temp=params.get(\"temperature\", 0.4)\n",
        "\n",
        "    idqueries= devData[0].tolist()\n",
        "    textqueries= devData[1].tolist()\n",
        "    labelqueries=devData[2].tolist()\n",
        "\n",
        "    exemplars = sampling_few_instances_multilabel(trainData, 6)\n",
        "\n",
        "    predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_few_prompt(task, exemplars, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt, ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        predictions.append(pred)\n",
        "        print(\"ID: \", idqueries[i],\"Ground Truth:\", labelqueries[i], \"Predicted: \", pred)\n",
        "\n",
        "    # Create dev output DataFrame\n",
        "    dev_df = pd.DataFrame({'id': idqueries,  'label': predictions, \"tweet\": textqueries, \"test_case\": [\"EXIST2025\"]*len(predictions) })\n",
        "    dev_df.to_csv('sexism_dev_predictions_task3_few.csv', index=False)\n",
        "    print(\"Prediction TASK3 completed. Results saved to sexism_dev_predictions_task3_few.csv\")\n",
        "\n",
        "    #Computing the quality measure for the subtask 3\n",
        "    icm= ICMWrapper(predictions, labelqueries, multi=True)\n",
        "    print(f\"\\nICM : {icm}\\n\\n\")\n",
        "\n",
        "    idqueries= testData[0].tolist()\n",
        "    textqueries= testData[1].tolist()\n",
        "    test_predictions=[]\n",
        "    for i in range(len(textqueries)):\n",
        "        query=textqueries[i]\n",
        "        filledPrompt=create_incontext_few_prompt(task, exemplars, query, context=None, role=role, output=output)\n",
        "        #print(\"PROMPT >>> \", filledPrompt)\n",
        "        response= perform_incontext_classification(model, tokenizer, filledPrompt,  ntokens, nseq, temp)\n",
        "        #print(\"#\"*25, \"ANSWER\", \"#\"*25)\n",
        "        #print(response)\n",
        "        pred=postprocessing(response)\n",
        "        test_predictions.append(pred)\n",
        "        print(\"ID: \", idqueries[i],  \"Predicted: \", pred)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': idqueries,\n",
        "        'label': test_predictions,\n",
        "        \"test_case\": [\"EXIST2025\"]*len(test_predictions) })\n",
        "    submission_df.to_csv('sexism_predictions_task3_few.csv', index=False)\n",
        "    print(\"Prediction TASK3 completed. Results saved to sexism_predictions_task3_few.csv\")\n",
        "\n",
        "params=dict()\n",
        "incontext_few_pipeline_task3(model, tokenizer,EnTrainTask3,  EnDevTask3, EnTestTask3, output_postprocessing_incontext_zero_s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4HhqbsURuaN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04022b46c14947f18f99e6c60d465fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063b0d61c1054b5db352e568314e3199": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a70e5ecf6e44cd98677e6afa7bb8fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bde47f9f0eec44b9b34576097986e985",
            "placeholder": "​",
            "style": "IPY_MODEL_5a4ce71e103f42c7b4a0d185f51057d7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0fadd5be48694c1383e501718052a829": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fb3b1edcbfe4fa1b3dee3fb5ab1325e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "105a0029e5df40f6aea9a90e83b14212": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "113011f632ca43b5a21786b214c8e64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a515bc8625d4ede94de204e94f06dfe",
              "IPY_MODEL_42657bb518814208a7a31fa501e466de",
              "IPY_MODEL_ccd6e306423a4c80af89cb3734e632e2"
            ],
            "layout": "IPY_MODEL_063b0d61c1054b5db352e568314e3199"
          }
        },
        "1873735b371c4ac495b45545e082c2d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53e1b2b2184e407cbbe35a2c72372689",
            "placeholder": "​",
            "style": "IPY_MODEL_53bcd23cf2214f8baae5a79ad2ff8e0c",
            "value": "generation_config.json: 100%"
          }
        },
        "1f4cfb687cad4908b6ad172f885bdfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46d762185c984592b0fc9e80c020d38a",
            "placeholder": "​",
            "style": "IPY_MODEL_5029f042f0c54333a539e5aa23699c5d",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "25405681348b4a3ca9e339d908dbbd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04022b46c14947f18f99e6c60d465fd7",
            "placeholder": "​",
            "style": "IPY_MODEL_90d9cb3afc5c4a3f84cd7a820cac179f",
            "value": " 2/2 [01:36&lt;00:00, 96.77s/it]"
          }
        },
        "26c69fc8f8ba45b188829a8329f7dc3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2816daf71873449499b94d191049afb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88c94abf3842469480ad069cc8d74f5a",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fb3b1edcbfe4fa1b3dee3fb5ab1325e",
            "value": 188
          }
        },
        "2fbbda15272349038767eac3366b64bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbda93dbc22498aa7484e62a7eec77b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35bb332464d5410196bdcfc26d24a7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f4cfb687cad4908b6ad172f885bdfba",
              "IPY_MODEL_504d7548083e40babc631e0090d3338c",
              "IPY_MODEL_53c807b2abac4162a5699d1c9a7847ae"
            ],
            "layout": "IPY_MODEL_382dce0877754294aede9a7384530367"
          }
        },
        "3741d19cd69c4b13b3deb2ab63c98094": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "382dce0877754294aede9a7384530367": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38791fae63d14d6d9cb28c5acab83dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9b6e69108344a9ebf7a529f1c832b0e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3741d19cd69c4b13b3deb2ab63c98094",
            "value": 2
          }
        },
        "3a515bc8625d4ede94de204e94f06dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f3900468644fa2a3d72776a1008a99",
            "placeholder": "​",
            "style": "IPY_MODEL_105a0029e5df40f6aea9a90e83b14212",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "3b65273d7a234188a4b6f515996584d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c69fc8f8ba45b188829a8329f7dc3c",
            "placeholder": "​",
            "style": "IPY_MODEL_58ed7a5fbb904deca51d6b2fcdb066f9",
            "value": "Fetching 2 files: 100%"
          }
        },
        "3cf31f3acdc5442587be16be47225125": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bf7f413fcf421ab8ae2805b5ce8c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7096493c7c14c2ab5743d41419346bd",
            "placeholder": "​",
            "style": "IPY_MODEL_d179f272ff79430980e0120943d3b26d",
            "value": " 188/188 [00:00&lt;00:00, 12.4kB/s]"
          }
        },
        "42657bb518814208a7a31fa501e466de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51ea0a07127442c7b85471406f39f642",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fc30abe168142a69202c70bd739fdce",
            "value": 3500296424
          }
        },
        "46d762185c984592b0fc9e80c020d38a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bcff74fb8714ba4b570ba4bd919f076": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e565d94d21e4c639fa2841a0b956255": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5029f042f0c54333a539e5aa23699c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "504d7548083e40babc631e0090d3338c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e565d94d21e4c639fa2841a0b956255",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e28b81e8b9d74350bb020a99709a3842",
            "value": 9976576152
          }
        },
        "51ea0a07127442c7b85471406f39f642": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53bcd23cf2214f8baae5a79ad2ff8e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53c807b2abac4162a5699d1c9a7847ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bcff74fb8714ba4b570ba4bd919f076",
            "placeholder": "​",
            "style": "IPY_MODEL_2fbbda15272349038767eac3366b64bc",
            "value": " 9.98G/9.98G [01:36&lt;00:00, 241MB/s]"
          }
        },
        "53e1b2b2184e407cbbe35a2c72372689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558f58e89f974f10a61a849c797f2500": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5841290a6c5f48fe900a5fcdde7ed07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58ed7a5fbb904deca51d6b2fcdb066f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a4ce71e103f42c7b4a0d185f51057d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e794156492f4e5099b4581df75ef601": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "638e362a7d774ba89f7a4fdefc823812": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aef6eac4f2b495985637863b4cc7184": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e313ea1b272c4b789820d3f180173146",
              "IPY_MODEL_8fd072227b664aa9a2226689a726a182",
              "IPY_MODEL_d9f25a6a4bd74c4e84f92188beb9db99"
            ],
            "layout": "IPY_MODEL_f8c77d4c81a3437289629727ca9facba"
          }
        },
        "6b89937737e34bc7824932cb86a313c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "728715459bda4e4281f35b99b89dab68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b552548d9e94cc4bb5cf9cf10325dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1873735b371c4ac495b45545e082c2d2",
              "IPY_MODEL_2816daf71873449499b94d191049afb4",
              "IPY_MODEL_41bf7f413fcf421ab8ae2805b5ce8c72"
            ],
            "layout": "IPY_MODEL_558f58e89f974f10a61a849c797f2500"
          }
        },
        "7b894e781fc345c18ca18994e7725c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fc30abe168142a69202c70bd739fdce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86f3900468644fa2a3d72776a1008a99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c94abf3842469480ad069cc8d74f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b8a6623a484a5c8a203b7cf1a8184d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a70e5ecf6e44cd98677e6afa7bb8fd5",
              "IPY_MODEL_db6d6cc647124009913b4204b04e344a",
              "IPY_MODEL_c40e72abe56448e5b3a02a80e7f1f1a1"
            ],
            "layout": "IPY_MODEL_e277108ffcbf4f0b8f606fd445dfacca"
          }
        },
        "8fd072227b664aa9a2226689a726a182": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_728715459bda4e4281f35b99b89dab68",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb97765678d34bbdbabe52b77328d66d",
            "value": 26788
          }
        },
        "90d9cb3afc5c4a3f84cd7a820cac179f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3226eb75a9f4b91a520d3eece3e7e42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde47f9f0eec44b9b34576097986e985": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c40e72abe56448e5b3a02a80e7f1f1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3226eb75a9f4b91a520d3eece3e7e42",
            "placeholder": "​",
            "style": "IPY_MODEL_cfe783facc2244af83837a4b9dd06a98",
            "value": " 2/2 [01:01&lt;00:00, 28.25s/it]"
          }
        },
        "c9b6e69108344a9ebf7a529f1c832b0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb97765678d34bbdbabe52b77328d66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccd6e306423a4c80af89cb3734e632e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cf31f3acdc5442587be16be47225125",
            "placeholder": "​",
            "style": "IPY_MODEL_5841290a6c5f48fe900a5fcdde7ed07f",
            "value": " 3.50G/3.50G [00:48&lt;00:00, 13.3MB/s]"
          }
        },
        "ceacdcc77bcf4e618846bbf09c65a4fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfe783facc2244af83837a4b9dd06a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d179f272ff79430980e0120943d3b26d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7096493c7c14c2ab5743d41419346bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f25a6a4bd74c4e84f92188beb9db99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceacdcc77bcf4e618846bbf09c65a4fc",
            "placeholder": "​",
            "style": "IPY_MODEL_7b894e781fc345c18ca18994e7725c51",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 2.53MB/s]"
          }
        },
        "db6d6cc647124009913b4204b04e344a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b89937737e34bc7824932cb86a313c9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fadd5be48694c1383e501718052a829",
            "value": 2
          }
        },
        "e277108ffcbf4f0b8f606fd445dfacca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e28b81e8b9d74350bb020a99709a3842": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e313ea1b272c4b789820d3f180173146": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fbda93dbc22498aa7484e62a7eec77b",
            "placeholder": "​",
            "style": "IPY_MODEL_638e362a7d774ba89f7a4fdefc823812",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "f8c77d4c81a3437289629727ca9facba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff4ec985d35a4ac5bd2a51627854faf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b65273d7a234188a4b6f515996584d2",
              "IPY_MODEL_38791fae63d14d6d9cb28c5acab83dc5",
              "IPY_MODEL_25405681348b4a3ca9e339d908dbbd63"
            ],
            "layout": "IPY_MODEL_5e794156492f4e5099b4581df75ef601"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

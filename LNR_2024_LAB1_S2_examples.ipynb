{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L6hBGMycAGVE"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IcZEnk89Ug4R"
      },
      "outputs": [],
      "source": [
        "# IF YOU USE GOOGLE COLAB AND WANT READ A FILE\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EZSqCAzMALDl"
      },
      "outputs": [],
      "source": [
        "# Defining the examples used in these activity\n",
        "toy_sentence_corpus = ['The cat sat on the mat',  'The dog chased the cat', 'The mat was soft and fluffy']\n",
        "\n",
        "ShortCorpus=[\"my husband is sick\", \"call me sexist\"]\n",
        "\n",
        "MyCorpus = [\"Yo it shirt he gave New York was funny.\",\n",
        "\"my husband is sick, homemade chicken soup loading husband.\",\n",
        "\"Witch raises wind to break up enemy’s lumber pound.\",\n",
        "\"i got a new shirt at work at it is the WORST material ever\",\n",
        "\"wave is so poor the girls can’t even dress up for Halloween\",\n",
        "\"The theory of paint indicates feelings of isolation in society.\",\n",
        "\"Wish sometimes I had access to the ever elusive cock carousel..\",\n",
        "\"off probation tomorrow ima be a free woman again, FUCK THE SYSTEM\"   ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1QwS2o9ZS-h"
      },
      "source": [
        "### EXAMPLE 1: Binary Bag of Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9262-7pIAfJ9",
        "outputId": "a76bdcc8-6a38-452c-d8a6-8331c982b2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'yo': 65, 'it': 32, 'shirt': 45, 'he': 24, 'gave': 19, 'new': 37, 'york': 66, 'was': 57, 'funny': 18, 'my': 36, 'husband': 26, 'is': 30, 'sick': 46, 'homemade': 25, 'chicken': 7, 'soup': 50, 'loading': 33, 'witch': 61, 'raises': 44, 'wind': 59, 'to': 54, 'break': 4, 'up': 56, 'enemy': 11, 'lumber': 34, 'pound': 42, 'got': 21, 'at': 2, 'work': 63, 'the': 52, 'worst': 64, 'material': 35, 'ever': 13, 'wave': 58, 'so': 47, 'poor': 41, 'girls': 20, 'can': 5, 'even': 12, 'dress': 9, 'for': 15, 'halloween': 23, 'theory': 53, 'of': 38, 'paint': 40, 'indicates': 29, 'feelings': 14, 'isolation': 31, 'in': 28, 'society': 48, 'wish': 60, 'sometimes': 49, 'had': 22, 'access': 0, 'elusive': 10, 'cock': 8, 'carousel': 6, 'off': 39, 'probation': 43, 'tomorrow': 55, 'ima': 27, 'be': 3, 'free': 16, 'woman': 62, 'again': 1, 'fuck': 17, 'system': 51}\n",
            "Vocabulary size:  67\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1]\n",
            "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "[0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "[0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "#Bag of Word, considering binary vectors (binary=True)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "vectorizer.fit(MyCorpus)\n",
        "X_bag_of_words = vectorizer.transform(MyCorpus)\n",
        "\n",
        "#Print the computed vocabulary\n",
        "print(vectorizer.vocabulary_)\n",
        "print(\"Vocabulary size: \", len(vectorizer.vocabulary_))\n",
        "for x in X_bag_of_words.toarray().astype(int):\n",
        "\tprint(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFCmWwdsc1IN"
      },
      "source": [
        "### EXAMPLE 2: Bag of Word without stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4afdza8ZA6gp",
        "outputId": "b8622ce2-2541-43ab-fd3a-5095b3c5e9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'yo': 43, 'shirt': 30, 'gave': 12, 'new': 24, 'york': 44, 'funny': 11, 'husband': 17, 'sick': 31, 'homemade': 16, 'chicken': 3, 'soup': 33, 'loading': 21, 'witch': 39, 'raises': 29, 'wind': 37, 'break': 1, 'enemy': 7, 'lumber': 22, 'pound': 27, 'got': 14, 'work': 41, 'worst': 42, 'material': 23, 'wave': 36, 'poor': 26, 'girls': 13, 'dress': 5, 'halloween': 15, 'theory': 34, 'paint': 25, 'indicates': 19, 'feelings': 8, 'isolation': 20, 'society': 32, 'wish': 38, 'access': 0, 'elusive': 6, 'cock': 4, 'carousel': 2, 'probation': 28, 'tomorrow': 35, 'ima': 18, 'free': 9, 'woman': 40, 'fuck': 10}\n",
            "Vocabulary size:  45\n",
            "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1]\n",
            "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0]\n",
            "[0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0]\n",
            "[0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0]\n",
            "[1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "#Bag of Word, considering the frequency of the terms (binary=False), and removing stopwords (stop_words=\"english\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", binary=False)\n",
        "vectorizer.fit(MyCorpus)\n",
        "X_bag_of_words = vectorizer.transform(MyCorpus)\n",
        "\n",
        "#Print the computed vocabulary\n",
        "print(vectorizer.vocabulary_)\n",
        "print(\"Vocabulary size: \", len(vectorizer.vocabulary_))\n",
        "for doc in X_bag_of_words.toarray().astype(int):\n",
        "\tprint(doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekd8xQHTjhJr"
      },
      "source": [
        "### EXAMPLE 3: Bigrams of characters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3FPpivhBzIi",
        "outputId": "539bbddb-75e6-4dbf-84ef-8830ae6c249b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' m': 3, 'my': 20, 'y ': 30, ' h': 1, 'hu': 13, 'us': 28, 'sb': 23, 'ba': 7, 'an': 6, 'nd': 21, 'd ': 10, ' i': 2, 'is': 15, 's ': 22, ' s': 4, 'si': 25, 'ic': 14, 'ck': 9, 'k ': 16, ' c': 0, 'ca': 8, 'al': 5, 'll': 18, 'l ': 17, 'me': 19, 'e ': 11, 'se': 24, 'ex': 12, 'xi': 29, 'st': 26, 't ': 27}\n",
            "Vocabulary size:  31\n",
            "[0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1]\n",
            "[1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "#Bag of Word, considering the frequency of the bigrams of characters (binary=False)\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
        "counts = ngram_vectorizer.fit_transform(ShortCorpus)\n",
        "print(ngram_vectorizer.vocabulary_)\n",
        "print(\"Vocabulary size: \", len(ngram_vectorizer.vocabulary_))\n",
        "for x in counts.toarray().astype(int):\n",
        "\tprint(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfNm_ax-luef"
      },
      "source": [
        "### EXAMPLE 4: 5-grams of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIrVdf8GCPil",
        "outputId": "4bdc0617-a6f4-4d26-80c0-f931845358d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' my ': 4, ' husb': 1, 'husba': 10, 'usban': 14, 'sband': 11, 'band ': 7, ' is ': 2, ' sick': 6, 'sick ': 13, ' call': 0, 'call ': 8, ' me ': 3, ' sexi': 5, 'sexis': 12, 'exist': 9, 'xist ': 15}\n",
            "Vocabulary size:  16\n",
            "[0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0]\n",
            "[1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "#Bag of Word, considering the frequency of the 5-grams of characters (binary=False)\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n",
        "counts=ngram_vectorizer.fit_transform(ShortCorpus)\n",
        "print(ngram_vectorizer.vocabulary_)\n",
        "print(\"Vocabulary size: \", len(ngram_vectorizer.vocabulary_))\n",
        "for doc in counts.toarray().astype(int):\n",
        "   print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLRlnM2LmihZ"
      },
      "source": [
        "### EXAMPLE 5: Bigrams of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK5MxH7ODIm_",
        "outputId": "eeda8079-a106-41df-baf7-2f8d053ffec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'yo shirt': 38, 'shirt gave': 25, 'gave new': 9, 'new york': 20, 'york funny': 39, 'husband sick': 13, 'sick homemade': 27, 'homemade chicken': 12, 'chicken soup': 2, 'soup loading': 28, 'loading husband': 17, 'witch raises': 34, 'raises wind': 24, 'wind break': 32, 'break enemy': 1, 'enemy lumber': 6, 'lumber pound': 18, 'got new': 11, 'new shirt': 19, 'shirt work': 26, 'work worst': 36, 'worst material': 37, 'wave poor': 31, 'poor girls': 22, 'girls dress': 10, 'dress halloween': 4, 'theory paint': 29, 'paint indicates': 21, 'indicates feelings': 15, 'feelings isolation': 7, 'isolation society': 16, 'wish access': 33, 'access elusive': 0, 'elusive cock': 5, 'cock carousel': 3, 'probation tomorrow': 23, 'tomorrow ima': 30, 'ima free': 14, 'free woman': 8, 'woman fuck': 35}\n",
            "Vocabulary size:  40\n",
            "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0]\n",
            "[0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0]\n",
            "[0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0]\n",
            "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0]\n",
            "[1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "#Bag of Word, considering the frequency of the bigrams of words (binary=False), and removing stopwords (stop_words=\"english\")\n",
        "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2,2), binary=False, stop_words=\"english\")\n",
        "data=ngram_vectorizer.fit_transform(MyCorpus)\n",
        "print(ngram_vectorizer.vocabulary_)\n",
        "print(\"Vocabulary size: \", len(ngram_vectorizer.vocabulary_))\n",
        "for doc in data.toarray().astype(int):\n",
        "   print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1fY00Pno-dc"
      },
      "source": [
        "### Example 6: Unigrams of words with TF-IDF (using Pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqCD5tWbDbLL",
        "outputId": "08c30612-d2c8-4020-8f32-c9dcf9735b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'yo': 43, 'shirt': 30, 'gave': 12, 'new': 24, 'york': 44, 'funny': 11, 'husband': 17, 'sick': 31, 'homemade': 16, 'chicken': 3, 'soup': 33, 'loading': 21, 'witch': 39, 'raises': 29, 'wind': 37, 'break': 1, 'enemy': 7, 'lumber': 22, 'pound': 27, 'got': 14, 'work': 41, 'worst': 42, 'material': 23, 'wave': 36, 'poor': 26, 'girls': 13, 'dress': 5, 'halloween': 15, 'theory': 34, 'paint': 25, 'indicates': 19, 'feelings': 8, 'isolation': 20, 'society': 32, 'wish': 38, 'access': 0, 'elusive': 6, 'cock': 4, 'carousel': 2, 'probation': 28, 'tomorrow': 35, 'ima': 18, 'free': 9, 'woman': 40, 'fuck': 10}\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.43 0.43 0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.36 0.   0.   0.\n",
            " 0.   0.   0.36 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.43 0.43]\n",
            "[0.   0.   0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.33 0.67 0.   0.   0.   0.33 0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.33 0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.38 0.   0.   0.   0.   0.   0.38 0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.38 0.   0.   0.   0.   0.38\n",
            " 0.   0.38 0.   0.   0.   0.   0.   0.   0.   0.38 0.   0.38 0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.43 0.   0.   0.   0.   0.   0.   0.   0.   0.43 0.36 0.   0.   0.\n",
            " 0.   0.   0.36 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.43\n",
            " 0.43 0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.45 0.   0.   0.   0.   0.   0.   0.   0.45\n",
            " 0.   0.45 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.45 0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.45 0.   0.   0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.41 0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.41 0.41 0.   0.   0.   0.   0.41 0.   0.\n",
            " 0.   0.   0.   0.   0.41 0.   0.41 0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.45 0.   0.45 0.   0.45 0.   0.45 0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.45 0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.41 0.41 0.   0.   0.\n",
            " 0.   0.   0.   0.   0.41 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.41 0.   0.   0.   0.   0.   0.   0.41 0.   0.   0.   0.   0.41 0.\n",
            " 0.   0.   0.  ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "pipe = Pipeline([('count', CountVectorizer(stop_words=\"english\", binary=False)), ('tfid', TfidfTransformer(use_idf=True))])\n",
        "counts=pipe.fit_transform(MyCorpus)\n",
        "\n",
        "print(pipe.named_steps['count'].vocabulary_)\n",
        "for doc in counts.toarray():\n",
        "   print(np.round(doc, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI00yP3TqAEu"
      },
      "source": [
        "### Example 6: Unigrams of words with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDtVaX3DDoHx",
        "outputId": "795e92d2-6cdc-4d3b-dc52-2b2ea4204705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'yo': 43, 'shirt': 30, 'gave': 12, 'new': 24, 'york': 44, 'funny': 11, 'husband': 17, 'sick': 31, 'homemade': 16, 'chicken': 3, 'soup': 33, 'loading': 21, 'witch': 39, 'raises': 29, 'wind': 37, 'break': 1, 'enemy': 7, 'lumber': 22, 'pound': 27, 'got': 14, 'work': 41, 'worst': 42, 'material': 23, 'wave': 36, 'poor': 26, 'girls': 13, 'dress': 5, 'halloween': 15, 'theory': 34, 'paint': 25, 'indicates': 19, 'feelings': 8, 'isolation': 20, 'society': 32, 'wish': 38, 'access': 0, 'elusive': 6, 'cock': 4, 'carousel': 2, 'probation': 28, 'tomorrow': 35, 'ima': 18, 'free': 9, 'woman': 40, 'fuck': 10}\n",
            "Vocabulary size:  40\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.43 0.43 0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.36 0.   0.   0.\n",
            " 0.   0.   0.36 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.43 0.43]\n",
            "[0.   0.   0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.33 0.67 0.   0.   0.   0.33 0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.33 0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.38 0.   0.   0.   0.   0.   0.38 0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.38 0.   0.   0.   0.   0.38\n",
            " 0.   0.38 0.   0.   0.   0.   0.   0.   0.   0.38 0.   0.38 0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.43 0.   0.   0.   0.   0.   0.   0.   0.   0.43 0.36 0.   0.   0.\n",
            " 0.   0.   0.36 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.43\n",
            " 0.43 0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.45 0.   0.   0.   0.   0.   0.   0.   0.45\n",
            " 0.   0.45 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.45 0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.45 0.   0.   0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.41 0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.41 0.41 0.   0.   0.   0.   0.41 0.   0.\n",
            " 0.   0.   0.   0.   0.41 0.   0.41 0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.45 0.   0.45 0.   0.45 0.   0.45 0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.45 0.   0.   0.\n",
            " 0.   0.   0.  ]\n",
            "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.41 0.41 0.   0.   0.\n",
            " 0.   0.   0.   0.   0.41 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            " 0.41 0.   0.   0.   0.   0.   0.   0.41 0.   0.   0.   0.   0.41 0.\n",
            " 0.   0.   0.  ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", binary=False, use_idf=True)\n",
        "count=tfidf_vectorizer.fit_transform(MyCorpus)\n",
        "print(tfidf_vectorizer.vocabulary_)\n",
        "print(\"Vocabulary size: \", len(ngram_vectorizer.vocabulary_))\n",
        "for doc in count.toarray():\n",
        "   print(np.round(doc, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXrgc9S9rkTt"
      },
      "source": [
        "### Example 7: LSA document representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je7usJOgET5X",
        "outputId": "48c0702d-849a-40f1-dc2d-b093e08b01e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After building the document-term (8, 45)\n",
            "After building the TFIDF matrix (8, 45)\n",
            "After Transforming the vectorial space with LSA (8, 5)\n",
            "[[ 7.93697112e-01 -2.52760702e-17  6.61962437e-18  2.73395866e-17\n",
            "  -9.84027880e-18]\n",
            " [ 2.78003714e-16  1.62353547e-01 -3.20751236e-01 -1.74639092e-01\n",
            "   6.01815585e-01]\n",
            " [-1.25706509e-16 -3.98466567e-01 -3.62191093e-01 -4.38472979e-01\n",
            "   2.21896860e-01]\n",
            " [ 7.93697112e-01 -1.07454571e-16 -2.49865463e-16  3.47402277e-16\n",
            "   3.83167894e-16]\n",
            " [-2.16864479e-16  5.90374040e-01 -4.78276370e-01 -2.62476217e-02\n",
            "   1.83704208e-01]\n",
            " [-8.56159443e-17  5.36166939e-01  2.65075654e-01  3.37214885e-01\n",
            "   1.52761243e-01]\n",
            " [ 9.50276324e-18 -2.05354950e-01  5.33951952e-01  5.54022143e-02\n",
            "   7.14889506e-01]\n",
            " [-3.82554885e-16 -3.69700651e-01 -4.26401433e-01  8.12262527e-01\n",
            "   1.42931962e-01]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "#Document-Term Matrix\n",
        "cv = CountVectorizer(stop_words=\"english\", binary=False)\n",
        "dtMatrix = cv.fit_transform(MyCorpus).toarray()\n",
        "print(\"After building the document-term\", dtMatrix.shape)\n",
        "featurenames = cv.get_feature_names_out()\n",
        "#print(featurenames)\n",
        "\n",
        "#Tf-idf Transformation\n",
        "tfidf = TfidfTransformer()\n",
        "tfidfMatrix = tfidf.fit_transform(dtMatrix).toarray()\n",
        "print(\"After building the TFIDF matrix\", tfidfMatrix.shape)\n",
        "\n",
        "#SVD\n",
        "#n_components is recommended to be 100 by Sklearn Documentation for LSA\n",
        "#http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "svd = TruncatedSVD(n_components = 5)\n",
        "svdMatrix = svd.fit_transform(tfidfMatrix)\n",
        "print(\"After Transforming the vectorial space with LSA\", svdMatrix.shape)\n",
        "print(svdMatrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwuM7BGBUg4X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

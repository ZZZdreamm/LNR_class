{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pY2x6TJTxZ7"
   },
   "source": [
    "<h1 align=\"center\">Lab 2: Sexism Identification in Twitter</h1>\n",
    "<h2 align=\"center\">Session 3. Transformers: Fine-tuning for multi-label classification\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv5aDZKaTxZ9"
   },
   "source": [
    "### Put your names here\n",
    "\n",
    "- Kacper Multan\n",
    "- Filip Polacik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Xc4hisRSCXf"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers --upgrade\n",
    "!pip install datasets accelerate\n",
    "!pip install PyEvALL\n",
    "!pip install scikit-learn\n",
    "!pip install peft\n",
    "# !pip install jupyter --upgrade\n",
    "# !pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBMTOhbPTxZ_"
   },
   "source": [
    "## Many libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0crkRHA5ivAN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Importing the required modules to use the ICM measure\n",
    "from pyevall.evaluation import PyEvALLEvaluation\n",
    "from pyevall.metrics.metricfactory import MetricFactory\n",
    "from pyevall.reports.reports import PyEvALLReport\n",
    "from pyevall.utils.utils import PyEvALLUtils\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4-2w1O_VMlq"
   },
   "outputs": [],
   "source": [
    "# IF YOU USE GOOGLE COLAB -> COLAB=True\n",
    "COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdNQeTP1UD7n"
   },
   "outputs": [],
   "source": [
    "if COLAB is True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_path = \"/content/drive/MyDrive/LNR/\"\n",
    "else:\n",
    "    base_path = \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLdRBXFmTxaA"
   },
   "source": [
    "## Import readerEXIST2025 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqRGK4XOTxaA"
   },
   "outputs": [],
   "source": [
    "library_path = os.path.join(base_path, \"Lab2-S1\")\n",
    "sys.path.append(library_path)\n",
    "from readerEXIST2025 import EXISTReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chPElJ_DPEKK"
   },
   "outputs": [],
   "source": [
    "# Path to the dataset, adapt this path wherever you have the dataset\n",
    "dataset_path = \"EXIST_2025_Dataset_V0.2/\"\n",
    "\n",
    "file_train = os.path.join(dataset_path, \"EXIST2025_training.json\")\n",
    "file_dev = os.path.join(dataset_path, \"EXIST2025_dev.json\")\n",
    "\n",
    "reader_train = EXISTReader(file_train)\n",
    "reader_dev = EXISTReader(file_dev)\n",
    "\n",
    "EnTrainTask3, EnDevTask3 = reader_train.get(lang=\"EN\", subtask=\"3\"), reader_dev.get(lang=\"EN\", subtask=\"3\")\n",
    "SpTrainTask3, SpDevTask3 = reader_train.get(lang=\"ES\", subtask=\"3\"), reader_dev.get(lang=\"ES\", subtask=\"3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i8bBADqn4tl"
   },
   "source": [
    "# Wrapper to compute ICM measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvEF2U0CPJBO"
   },
   "outputs": [],
   "source": [
    "def ICMWrapper(pred, labels, multi=False, ids=None):\n",
    "    test = PyEvALLEvaluation()\n",
    "    metrics = [MetricFactory.ICM.value]\n",
    "    params = {}\n",
    "    fillLabel = None\n",
    "    if multi:\n",
    "        params[PyEvALLUtils.PARAM_FORMAT] = PyEvALLUtils.PARAM_VALUE_FORMAT_MULTILABEL\n",
    "        fillLabel = \"O\"\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as pred_file, \\\n",
    "         tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as gold_file:\n",
    "        pred_dict = {}\n",
    "        gold_dict = {}\n",
    "        for i, (p, l) in enumerate(zip(pred, labels)):\n",
    "            curr_id = ids[i] if ids is not None else str(i)\n",
    "            pred_dict[curr_id] = p\n",
    "            gold_dict[curr_id] = l\n",
    "        \n",
    "        PyEvALLUtils.write_json_file(pred_file.name, pred_dict)\n",
    "        PyEvALLUtils.write_json_file(gold_file.name, gold_dict)\n",
    "        \n",
    "        report = test.evaluate(gold_file.name, pred_file.name, metrics, params, fillLabel)\n",
    "    \n",
    "    os.unlink(pred_file.name)\n",
    "    os.unlink(gold_file.name)\n",
    "    \n",
    "    icm_score = report.get_metrics()[MetricFactory.ICM.value]\n",
    "    return icm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for multi-label classification\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mlb = MultiLabelBinarizer(classes=[\n",
    "            'IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'OBJECTIFICATION',\n",
    "            'SEXUAL-VIOLENCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE'\n",
    "        ])\n",
    "        self.labels = self.mlb.fit_transform([d['labels_task3'] for d in data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.FloatTensor(self.labels[idx]),\n",
    "            'id': self.data[idx]['id']\n",
    "        }\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    # Apply sigmoid and threshold at 0.5 for multi-label\n",
    "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    # Compute ICM\n",
    "    pred_labels = [list(np.array(mlb.classes_)[p.astype(bool)]) for p in preds]\n",
    "    true_labels = [list(np.array(mlb.classes_)[l.astype(bool)]) for l in labels]\n",
    "    icm = ICMWrapper(pred_labels, true_labels, multi=True, ids=None)\n",
    "    # Compute Macro-F1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    return {\n",
    "        'icm': icm,\n",
    "        'macro_f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Pipeline for training and evaluation\n",
    "def sexism_classification_pipeline_task3(train_data, dev_data, model_name, technique='fine-tune', config='conservative'):\n",
    "    global mlb  # For compute_metrics\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=5,\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA if specified\n",
    "    if technique == 'lora':\n",
    "        lora_config = LoraConfig(\n",
    "            r=8 if config == 'conservative' else 16,\n",
    "            lora_alpha=16 if config == 'conservative' else 32,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\"\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(train_data, tokenizer)\n",
    "    dev_dataset = CustomDataset(dev_data, tokenizer)\n",
    "    mlb = train_dataset.mlb  # For compute_metrics\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_name.split('/')[-1]}_{technique}_{config}\",\n",
    "        learning_rate=2e-5 if config == 'conservative' else 5e-5,\n",
    "        per_device_train_batch_size=16 if config == 'conservative' else 8,\n",
    "        per_device_eval_batch_size=16 if config == 'conservative' else 8,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01 if config == 'conservative' else 0.1,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "        gradient_accumulation_steps=2 if config == 'aggressive' else 1\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3 if config == 'conservative' else 2)]\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    # Compute per-label F1 scores\n",
    "    predictions = trainer.predict(dev_dataset)\n",
    "    logits = predictions.predictions\n",
    "    labels = predictions.label_ids\n",
    "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    per_label_f1 = precision_recall_fscore_support(labels, preds, average=None)[2]\n",
    "    per_label_f1_dict = {label: f1 for label, f1 in zip(mlb.classes_, per_label_f1)}\n",
    "    \n",
    "    # Store training time\n",
    "    training_time = time.time() - start_time\n",
    "    results['training_time'] = training_time\n",
    "    results['per_label_f1'] = per_label_f1_dict\n",
    "    results['epochs'] = trainer.state.epoch\n",
    "    results['time_per_epoch'] = training_time / trainer.state.epoch if trainer.state.epoch > 0 else training_time\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train English Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# English: BERT\n",
    "print(\"Training BERT English (Fine-Tune, Conservative)\")\n",
    "bert_en_ft_cons_model, bert_en_ft_cons_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"bert-base-uncased\", technique=\"fine-tune\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'BERT', 'technique': 'Fine-Tune', 'config': 'Conservative',\n",
    "    **bert_en_ft_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training BERT English (Fine-Tune, Aggressive)\")\n",
    "bert_en_ft_aggr_model, bert_en_ft_aggr_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"bert-base-uncased\", technique=\"fine-tune\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'BERT', 'technique': 'Fine-Tune', 'config': 'Aggressive',\n",
    "    **bert_en_ft_aggr_results\n",
    "})\n",
    "\n",
    "print(\"Training BERT English (LoRA, Conservative)\")\n",
    "bert_en_lora_cons_model, bert_en_lora_cons_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"bert-base-uncased\", technique=\"lora\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'BERT', 'technique': 'LoRA', 'config': 'Conservative',\n",
    "    **bert_en_lora_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training BERT English (LoRA, Aggressive)\")\n",
    "bert_en_lora_aggr_model, bert_en_lora_aggr_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"bert-base-uncased\", technique=\"lora\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'BERT', 'technique': 'LoRA', 'config': 'Aggressive',\n",
    "    **bert_en_lora_aggr_results\n",
    "})\n",
    "\n",
    "# English: RoBERTa\n",
    "print(\"Training RoBERTa English (Fine-Tune, Conservative)\")\n",
    "roberta_en_ft_cons_model, roberta_en_ft_cons_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"roberta-base\", technique=\"fine-tune\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'RoBERTa', 'technique': 'Fine-Tune', 'config': 'Conservative',\n",
    "    **roberta_en_ft_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training RoBERTa English (Fine-Tune, Aggressive)\")\n",
    "roberta_en_ft_aggr_model, roberta_en_ft_aggr_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"roberta-base\", technique=\"fine-tune\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'RoBERTa', 'technique': 'Fine-Tune', 'config': 'Aggressive',\n",
    "    **roberta_en_ft_aggr_results\n",
    "})\n",
    "\n",
    "print(\"Training RoBERTa English (LoRA, Conservative)\")\n",
    "roberta_en_lora_cons_model, roberta_en_lora_cons_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"roberta-base\", technique=\"lora\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'RoBERTa', 'technique': 'LoRA', 'config': 'Conservative',\n",
    "    **roberta_en_lora_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training RoBERTa English (LoRA, Aggressive)\")\n",
    "roberta_en_lora_aggr_model, roberta_en_lora_aggr_results = sexism_classification_pipeline_task3(\n",
    "    EnTrainTask3, EnDevTask3, model_name=\"roberta-base\", technique=\"lora\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'English', 'model': 'RoBERTa', 'technique': 'LoRA', 'config': 'Aggressive',\n",
    "    **roberta_en_lora_aggr_results\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Spanish Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish: BETO\n",
    "print(\"Training BETO Spanish (Fine-Tune, Conservative)\")\n",
    "beto_es_ft_cons_model, beto_es_ft_cons_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"dccuchile/bert-base-spanish-wwm-uncased\", technique=\"fine-tune\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'BETO', 'technique': 'Fine-Tune', 'config': 'Conservative',\n",
    "    **beto_es_ft_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training BETO Spanish (Fine-Tune, Aggressive)\")\n",
    "beto_es_ft_aggr_model, beto_es_ft_aggr_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"dccuchile/bert-base-spanish-wwm-uncased\", technique=\"fine-tune\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'BETO', 'technique': 'Fine-Tune', 'config': 'Aggressive',\n",
    "    **beto_es_ft_aggr_results\n",
    "})\n",
    "\n",
    "print(\"Training BETO Spanish (LoRA, Conservative)\")\n",
    "beto_es_lora_cons_model, beto_es_lora_cons_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"dccuchile/bert-base-spanish-wwm-uncased\", technique=\"lora\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'BETO', 'technique': 'LoRA', 'config': 'Conservative',\n",
    "    **beto_es_lora_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training BETO Spanish (LoRA, Aggressive)\")\n",
    "beto_es_lora_aggr_model, beto_es_lora_aggr_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"dccuchile/bert-base-spanish-wwm-uncased\", technique=\"lora\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'BETO', 'technique': 'LoRA', 'config': 'Aggressive',\n",
    "    **beto_es_lora_aggr_results\n",
    "})\n",
    "\n",
    "# Spanish: RoBERTa-BNE\n",
    "print(\"Training RoBERTa-BNE Spanish (Fine-Tune, Conservative)\")\n",
    "roberta_bne_es_ft_cons_model, roberta_bne_es_ft_cons_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"PlanTL-GOB-ES/roberta-base-bne\", technique=\"fine-tune\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'RoBERTa-BNE', 'technique': 'Fine-Tune', 'config': 'Conservative',\n",
    "    **roberta_bne_es_ft_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training RoBERTa-BNE Spanish (Fine-Tune, Aggressive)\")\n",
    "roberta_bne_es_ft_aggr_model, roberta_bne_es_ft_aggr_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"PlanTL-GOB-ES/roberta-base-bne\", technique=\"fine-tune\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'RoBERTa-BNE', 'technique': 'Fine-Tune', 'config': 'Aggressive',\n",
    "    **roberta_bne_es_ft_aggr_results\n",
    "})\n",
    "\n",
    "print(\"Training RoBERTa-BNE Spanish (LoRA, Conservative)\")\n",
    "roberta_bne_es_lora_cons_model, roberta_bne_es_lora_cons_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"PlanTL-GOB-ES/roberta-base-bne\", technique=\"lora\", config=\"conservative\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'RoBERTa-BNE', 'technique': 'LoRA', 'config': 'Conservative',\n",
    "    **roberta_bne_es_lora_cons_results\n",
    "})\n",
    "\n",
    "print(\"Training RoBERTa-BNE Spanish (LoRA, Aggressive)\")\n",
    "roberta_bne_es_lora_aggr_model, roberta_bne_es_lora_aggr_results = sexism_classification_pipeline_task3(\n",
    "    SpTrainTask3, SpDevTask3, model_name=\"PlanTL-GOB-ES/roberta-base-bne\", technique=\"lora\", config=\"aggressive\"\n",
    ")\n",
    "all_results.append({\n",
    "    'language': 'Spanish', 'model': 'RoBERTa-BNE', 'technique': 'LoRA', 'config': 'Aggressive',\n",
    "    **roberta_bne_es_lora_aggr_results\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Simplify per-label F1 into separate columns\n",
    "for label in ['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'OBJECTIFICATION',\n",
    "              'SEXUAL-VIOLENCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE']:\n",
    "    results_df[f'f1_{label}'] = results_df['per_label_f1'].apply(lambda x: x.get(label, 0.0))\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('sexism_classification_results.csv', index=False)\n",
    "\n",
    "# Display summary table\n",
    "summary_df = results_df[['language', 'model', 'technique', 'config', 'icm', 'macro_f1', 'time_per_epoch']]\n",
    "print(\"\\nResults Summary:\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Group by language\n",
    "for lang in ['English', 'Spanish']:\n",
    "    print(f\"\\n{lang}\\n\")\n",
    "    for tech in ['Fine-Tune', 'LoRA']:\n",
    "        print(f\"{tech}\")\n",
    "        lang_tech_df = summary_df[(summary_df['language'] == lang) & (summary_df['technique'] == tech)]\n",
    "        for _, row in lang_tech_df.iterrows():\n",
    "            print(f\"\\t{row['model']} ({row['config']}): ICM:{row['icm']:.4f}  Macro-F1:{row['macro_f1']:.4f} ({row['time_per_epoch']:.2f}s per epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot: Macro-F1 by Model, Technique, and Language\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='macro_f1', y='model', hue='technique', style='config', size='config',\n",
    "            sizes={'Conservative': 1, 'Aggressive': 2}, palette='viridis',\n",
    "            data=results_df)\n",
    "plt.title('Macro-F1 Comparison by Model, Technique, and Configuration')\n",
    "plt.xlabel('Macro-F1')\n",
    "plt.ylabel('Model')\n",
    "plt.legend(title='Technique / Config')\n",
    "plt.tight_layout()\n",
    "plt.savefig('macro_f1_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Bar Plot: ICM by Model, Technique, and Language\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='icm', y='model', hue='technique', style='config', size='config',\n",
    "            sizes={'Conservative': 1, 'Aggressive': 2}, palette='magma',\n",
    "            data=results_df)\n",
    "plt.title('ICM Comparison by Model, Technique, and Configuration')\n",
    "plt.xlabel('ICM')\n",
    "plt.ylabel('Model')\n",
    "plt.legend(title='Technique / Config')\n",
    "plt.tight_layout()\n",
    "plt.savefig('icm_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Per-Label F1 Scores Heatmap\n",
    "per_label_df = results_df[['language', 'model', 'technique', 'config',\n",
    "                          'f1_IDEOLOGICAL-INEQUALITY', 'f1_STEREOTYPING-DOMINANCE',\n",
    "                          'f1_OBJECTIFICATION', 'f1_SEXUAL-VIOLENCE',\n",
    "                          'f1_MISOGYNY-NON-SEXUAL-VIOLENCE']]\n",
    "per_label_df = per_label_df.melt(id_vars=['language', 'model', 'technique', 'config'],\n",
    "                                 var_name='label', value_name='f1')\n",
    "per_label_df['label'] = per_label_df['label'].str.replace('f1_', '')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "pivot_table = per_label_df.pivot_table(values='f1', index=['language', 'model', 'technique', 'config'],\n",
    "                                      columns='label')\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "plt.title('Per-Label F1 Scores')\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_label_f1_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# Note: Learning curves require trainer logs, which are not easily accessible here.\n",
    "# To add learning curves, modify the pipeline to save trainer.state.log_history to a file.\n",
    "# Example code (if logs are available):\n",
    "# plt.plot(logs['epoch'], logs['eval_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}